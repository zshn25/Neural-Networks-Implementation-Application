{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 4: Machine Learning Fundamentals & Linear Regression (Deadline: 01 Dec 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Fundamentals(7 points)\n",
    "For theoretical tasks you are encouraged to write in $\\\\LaTeX$. Jupyter notebooks support them by default. For reference, please have a look at the examples in this short excellent guide: [Typesetting Equations](http://nbviewer.jupyter.org/github/ipython/ipython/blob/3.x/examples/Notebook/Typesetting%20Equations.ipynb)\n",
    "\n",
    "Alternatively, you can upload the solutions in the written form as images and paste them inside the cells. But if you do this, **make sure** that the images are of high quality, so that we can read them without any problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return (sigmoid(x) * (1.0 - sigmoid(x)))\n",
    "\n",
    "xs = np.linspace(-5.0, 5.0, 100)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.axhline(0, ls='--', c='black', lw=.5)\n",
    "ax1.axvline(0, ls='--', c='black', lw=.5)\n",
    "ax2.axhline(0, ls='--', c='black', lw=.5)\n",
    "ax2.axvline(0, ls='--', c='black', lw=.5)\n",
    "ax1.plot(xs, sigmoid(xs), c='red')\n",
    "ax1.plot(xs, sigmoid(-xs), c='purple', ls='--')\n",
    "\n",
    "ax2.plot(xs, dsigmoid(xs), c='blue')\n",
    "ax1.set_title('Sigmoid')\n",
    "ax2.set_title('Derivative');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Sigmoid Function (1.5 points)\n",
    "The special case of the logistic function is the *sigmoid function* which is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\sigma(a) = \\frac{1}{1 + e^{-a}}\n",
    "\\end{equation*}\n",
    "\n",
    "a) Compute its gradient analytically. (0.5 points)\n",
    "\n",
    "###### Answer\n",
    "\n",
    "$$\\frac{\\partial \\sigma(a)}{\\partial a} = \\frac{\\partial}{\\partial a} \\frac{1}{1 + e^{-a}} = \\frac{\\partial}{\\partial a} (1 + e^{-a})^{-1} = (1 + e^{-a})^{-2} e^{-a} = \\frac{e^{-a}}{(1 + e^{-a})^2} = -\\frac{1}{1 + e^{-a}} \\cdot \\frac{e^{-a}}{1 + e^{-a}} =\\\\ -\\frac{1}{1 + e^{-a}} \\cdot \\frac{e^{-a}}{e^{a-a} + e^{-a}} = \\frac{1}{1 + e^{-a}} \\cdot \\frac{e^{-a}}{(e^a + 1) e^{-a}} = \\frac{1}{1 + e^{-a}} \\cdot \\frac{1}{e^a + 1} = \\sigma(a) \\cdot \\sigma(-a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) What are the inherent properties that you observe from the above computed gradient? (0.5 points) <br />\n",
    "   *Hint: Think about how would the gradient signal be for the whole domain of the sigmoid function*\n",
    "   \n",
    "###### Answer\n",
    "\n",
    "From the above computation we can see, that we finally end with $\\sigma(a) \\cdot \\sigma(-a)$. $\\sigma(a)$ is zero or close to zero for (very) negative values and close to 1 for (very) positive values and $\\sigma(-a)$ would be the mirrored version of that. So mupltiplying these two functions would result in a function which is close to zero for (very) negative and (very) positive values and has a peek at 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Prove that the sigmoid function is symmetric. (0.5 points)\n",
    "\n",
    "###### Answer\n",
    "\n",
    "The arguments of the sigmoid function do not do any operations between them. So one can just interchange them without changing anything on the function output:\n",
    "\n",
    "$$\\sigma([a_1, a_2, ... , a_n]) = \\sigma([a_2, ..., a_n, a_1]) = \\sigma([a_2, a_1, ..., a_n]) = \\sigma([a_n, a_1, ..., a_2])$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Points**: 1.5 of 1.5\n",
    "**Comments**:\n",
    "- The question in part $b)$ was missleading. It should state: Prove that the sigmoid function is rotational symmetric.\n",
    "- By showing that $1- \\sigma(x) = \\sigma(x)$, one can prove that $\\sigma(x) - 0.5$ is an odd function. Which is the desired symmetric property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Regularization (3.5 points)\n",
    "\n",
    "In the lecture, we've seen that we can add a *regularizer* to our cost function to avoid *over or underfitting*. For example, consider the following training criterion for linear regression:\n",
    "\n",
    "\\begin{equation*}\n",
    "  J(\\textbf{w}) = \\frac{1}{m}\\sum_{i=1}^{m} \\Vert\\hat{y}^{(i)} - y^{(i)}\\Vert^{2} + \\lambda\\Omega(\\textbf{w})\n",
    "\\end{equation*}\n",
    "where $\\Omega(\\textbf{w}) = \\textbf{w}^{T}\\textbf{w}$ is the regularizer.\n",
    "\n",
    "a) In the above criterion, what is the role of the regularization parameter $\\lambda$ on the regularizer (i.e. parameters of our model) while minimizing $J(\\textbf{w})$? (1.0 point)\n",
    "\n",
    "###### Answer\n",
    "\n",
    "The regularization parameter decides how much the regularizer should be weighted to our final model. I.e. if the weight is higher, then the model is less prone to overfitting because the the model does not put as much weight on the training data. However, if it is too high then the model might underfit. If it is too low then the model might overfit because then the model considers the training data too much and new test data will have a much higher error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Is $\\lambda$ the model parameter or a hyperparameter? Justify.(0.5 points)\n",
    "\n",
    "###### Answer\n",
    "\n",
    "$\\lambda$ is a hyperparameter because it is not changed during the training, unlike $\\textbf{w}$. It is also not supposed to change during training because then it would not be able to underfit the model to the training data anymore and would basically become zero because of our loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Derive the closed form solution for the weights ($\\textbf{w}$) in the above criterion.(2.0 points)\n",
    "\n",
    "###### Answer\n",
    "\n",
    "$$J(\\textbf{w}) = \\frac{1}{m} \\|X\\textbf{w} - \\textbf{y}\\|^2 + \\lambda\\Omega(\\textbf{w}) = \\frac{1}{m}\\|X\\textbf{w} - \\textbf{y}\\|^2 + \\lambda \\textbf{w}^{T}\\textbf{w} = \\frac{1}{m}\\|\\tilde{X}\\textbf{w} - \\tilde{\\textbf{y}}\\|^2 \\text{ , with } \\tilde{X} = \\begin{pmatrix}X \\\\ \\sqrt{\\lambda} I\\end{pmatrix} \\text{ and } \\tilde{\\textbf{y}} = \\begin{pmatrix}y\\\\0\\end{pmatrix}$$\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{1}{m}\\|\\tilde{X}\\textbf{w} - \\tilde{\\textbf{y}}\\|^2 = \\frac{1}{m}(\\tilde{X}\\textbf{w} - \\tilde{\\textbf{y}})^T (\\tilde{X}\\textbf{w} - \\tilde{\\textbf{y}}) &= 0 \\Leftrightarrow\\\\\n",
    "\\textbf{w}^T \\tilde{X}^T \\tilde{X} \\textbf{w} - 2 \\textbf{w}^T \\tilde{X} \\tilde{\\textbf{y}} + \\tilde{\\textbf{y}}^T \\tilde{\\textbf{y}}&= 0 \\Leftrightarrow\\\\\n",
    "2 \\tilde{X}^T \\tilde{X} \\textbf{w} - 2 \\tilde{X}^T \\tilde{\\textbf{y}} &= 0 \\Leftrightarrow \\\\\n",
    "\\textbf{w} &= (\\tilde{X}^T \\tilde{X})^{-1} \\tilde{X}^T \\tilde{\\textbf{y}} \\Leftrightarrow \\\\\n",
    "\\textbf{w} &= (X^T X + \\lambda I)^{-1} X^T \\textbf{y}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Points**: 3.5 of 3.5\n",
    "**Comments**:\n",
    "- Please present your solution in the tutorial. Thanks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3. Maximum Likelihood Estimation (MLE) (2 points)\n",
    "Consider the density function of a ***univariate Gaussian distribution***\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    " p(x;\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu)^{2}\\right)\n",
    "\\end{equation*}\n",
    "where $\\mu$ is the $\\textit{mean}$ and $\\sigma^{2}$ is the $\\textit{variance}$. \n",
    "\n",
    "Let's say you're given *N* samples (i.e. $x_1, x_2, x_3, ..., x_N$) which are drawn from the above stated distribution. Also, you can assume that these samples are **i.i.d** (i.e. [independent and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)).\n",
    "\n",
    "Now, please derive the *MLE step-by-step* for:\n",
    "\n",
    "###### Answers\n",
    "\n",
    "The MLE is given by\n",
    "\n",
    "$$\\mathcal{L}(\\mu, \\sigma) = \\prod_{i=1}^n p(x_i;\\mu,\\sigma^2) = \\left(\\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^{2}\\right) \\Leftrightarrow \\log(\\mathcal{L}(\\mu, \\sigma)) = \\frac{n}{2} \\log\\left(\\frac{1}{2\\pi\\sigma^2}\\right) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^{2}$$\n",
    "\n",
    "a) *mean* $(\\mu)$. (1.0 point)\n",
    "\n",
    "For the mean we take the derivative of the above equation with respect to $\\mu$ and equate it to zero:\n",
    "\n",
    "$$0 = \\frac{\\partial}{\\partial \\mu} \\log(\\mathcal{L}(\\mu, \\sigma)) = 0 - \\frac{-2 \\sum_{i=1}^n(x_i - \\mu)}{2\\sigma^2} = \\frac{\\sum_{i=1}^n(x_i - \\mu)}{\\sigma^2} = n \\left(\\sum_{i=1}^n \\frac{x_i}{n} - \\mu\\right) \\Leftrightarrow \\mu = \\sum_{i=1}^n \\frac{x_i}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) *variance* $(\\sigma^2)$. (1.0 point)\n",
    "\n",
    "For the variance we take the derivative of the MLE equation with respect to $\\sigma$ and equate it to zero:\n",
    "\n",
    "$$0 = \\frac{\\partial}{\\partial \\sigma} \\log(\\mathcal{L}(\\mu, \\sigma)) = -\\frac{n}{\\sigma} + \\sigma^{-3} \\sum_{i=1}^n (x_i - \\mu)^2 = -\\frac{n}{\\sigma} + \\frac{\\sum_{i=1}^n (x_i - \\mu)^2}{\\sigma^3} = -\\frac{\\sigma^2 n}{\\sigma^3} + \\frac{\\sum_{i=1}^n (x_i - \\mu)^2}{\\sigma^3} = \\\\\n",
    "\\frac{-\\sigma^2 n + \\sum_{i=1}^n (x_i - \\mu)^2}{\\sigma^3} = -\\sigma^2 n + \\sum_{i=1}^n (x_i - \\mu)^2 \\Leftrightarrow \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Points**: 2.0 of 2.0\n",
    "**Comments**:\n",
    "- None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression (13 points)\n",
    "\n",
    "#### 1. Introduction\n",
    "As we have seen in first assignment sheet, when we have one independent (or explanatory) variable and a scalar dependent variable, it is called **simple linear regression**.\n",
    "But, when there are more than one explanatory variable (i.e. $x^{(1)}, x^{(2)}, ...,x^{(k)}$), and a single scalar dependent variable (*y*), then it's called $\\textit{multiple linear regression}$. (Please don't confuse this with *multivariate linear regression* where we predict more than one (correlated) dependent variable.)\n",
    "\n",
    "Here, we will implement a **multiple linear regression** model in Python/NumPy using the *Gradient Descent* algorithm. Particularly, we will be using $\\textit{stochastic gradient descent}$ (*SGD*) where one performs the update step using a small set of training samples of size *batch_size* which we will set to 64. This is again a hyperparameter but in this exercise we will just use a fixed batch-size of *64* (i.e. we go through the training samples sampling 64 at a time and perform gradient descent.) Such a procedure is sometimes called *mini-batch gradient descent* in the deep learning community.\n",
    "\n",
    "Going through all the training samples *once* is called an **epoch**. Ideally, the algorithm has to go through multiple epochs over the training samples, each time shuffling it, until a convergence criterion has been satisfied. <br />\n",
    "\n",
    "Here, we will set a *tolerance value* for the difference in error (i.e. change in MSE values between subsequent epochs) that we will accept. Once this difference falls below the *tolerance value*, we terminate our training phase and return the parameters. \n",
    "\n",
    "We repeat the above training procedure for all possible hyperparameter combinations. Later on, using these parameters (*i.e. weight vectors*), we compute the prediction for validation data and the corresponding MSE values. And then, we pick the hyperparameter combination which yielded the least MSE.\n",
    "\n",
    "As a next step, we will combine training data and validation data and make it as our *new training data*. We keep the test data as it is. Using the hyperparameter combination (for the least MSE) that we found above, we train the model again with the *new training data* and obtain the parameter (*i.e. weight vector*) after convergence according to our *tolerance value*.\n",
    "\n",
    "Phew! That will be our much desired *weight vector*. This is then used on the *test data*, which has not been seen by our algorithm so far, to make a prediction. The resulting MSE value will be the so-called [*generalization error*](https://en.wikipedia.org/wiki/Generalization_error).\n",
    "\n",
    "It is this *generalization error* that we want it to be as low as possible for *unseen data* (implies that we can achieve higher accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Dataset\n",
    "For our task, we will be using the *Wine Quality* dataset and predict the quality of white wine based on 11 features such as acidity, citric acid content, residual sugar etc. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get data\n",
    "data_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'\n",
    "data = pd.read_csv(data_url, sep=';')\n",
    "\n",
    "# inspect data\n",
    "print(data.head())\n",
    "print(data.shape)\n",
    "\n",
    "# data as np array\n",
    "data_npr = data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Loss function\n",
    "We will use a *regularized* form of the MSE loss function. In matrix form it can be written as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "    J(\\textbf{w}) = \\frac{1}{2} \\Vert{X\\textbf{w}-\\textbf{y}}\\Vert^{2} + \\frac{\\lambda}{2}\\Vert{\\textbf{w}}\\Vert^{2}\n",
    "\\end{equation*}\n",
    "\n",
    "It's important to note that, in the above equation, $X$, called *design matrix*, is the horizontal concatenation of shape *(batch_size, num_features)* according to the *order* of the polynomial. To make things easier, you can add the *bias* term as the first column of $X$. Take care to have the *weight* vector $\\textbf{w}$ with matching dimensions.\n",
    "\n",
    "$\\textit{Hint}$: see [Design_matrix#Multiple_regression](https://en.wikipedia.org/wiki/Design_matrix#Multiple_regression) for how $X$ with 2 features looks like for $1^{st}$ degree polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Derive the gradient (w.r.t $\\textbf{w}$) for the regularized loss function given in **3**. (1.0 point)\n",
    "\n",
    "##### Answer\n",
    "\n",
    "$$\\frac{\\partial J(\\textbf{w})}{\\partial \\textbf{w}} = X^T \\|X \\textbf{w} - \\textbf{y}\\| + \\lambda \\textbf{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Matrix format for higher order polynomial\n",
    "\n",
    "Written in matrix form, linear regression model for second order would look like: <br />\n",
    "$$\\hat{\\textbf{y}} = X\\textbf{w}_{1} + X^{2}\\textbf{w}_{2} + \\textbf{b}$$\n",
    "\n",
    "where $X^{2}$ is the element-wise squaring of the original design matrix $X$, $\\textbf{w}_1$ and $\\textbf{w}_2$ are the *weight* vectors, and **b** is the *bias* vector.\n",
    "\n",
    "a) Now, please write down the matrix format for a $9^{th}$ order linear regression model (0.5 points)\n",
    "\n",
    "##### Answer\n",
    "\n",
    "$$\\hat{\\textbf{y}} = \\textbf{b} + \\sum_{i=1}^p X^{i} \\textbf{w}_i \\text{ , with }p = 9$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Hyperparameters\n",
    "we will experiment with three hyperparameters:\n",
    "\n",
    "i) regularization parameter $\\lambda$ <br />\n",
    "ii) learning rate $\\epsilon$ <br />\n",
    "iii) order of polynomial *p*\n",
    "\n",
    "And do a grid search over the values that these hyperparameters can take in order to select the best combination (i.e. the one that achieves lowest test error). This approach is called **hyperparameter optimization or tuning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_order = [1, 5, 9]\n",
    "learning_rates = [1e-5, 1e-8]\n",
    "lambdas = [0.1, 0.8]\n",
    "\n",
    "#hyperparams combination\n",
    "comb_gen = itertools.product(*(polynomial_order, learning_rates, lambdas))\n",
    "hparams_comb = list(comb_gen)\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Normalization\n",
    "First of all, inspect the data, and understand its structure and features. Ideally, before starting to train our learning algorithm, we would want the data to be normalized. Here, we normalize the data (i.e. normalize each column) using the formula:\n",
    "\n",
    "\\begin{equation*}\n",
    "  norm\\_x_i = \\frac{x_i - min(x)}{max(x) - min(x)}\n",
    "\\end{equation*}\n",
    "where $x_i$ is the $i^{th}$ sample in feature $x$\n",
    "\n",
    "a) Complete the following function which performs normalization (i.e. normalizes columns of $X$). (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_normalization(data):\n",
    "    # TODO: implement\n",
    "    col_mins = data.values.min(0)\n",
    "    data_normalized = (data.values - col_mins)/(data.values.max(0) - col_mins)\n",
    "    return data_normalized\n",
    "\n",
    "\n",
    "# perform data normalization\n",
    "data_normalized = data_normalization(data)\n",
    "data_npr = data_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data_npr):\n",
    "    # (in-place) shuffling of data_npr along axis 0\n",
    "    np.random.shuffle(data_npr)\n",
    "\n",
    "    n_tr = 3898\n",
    "    n_va = n_tr + 500\n",
    "    n_te = n_va + 500\n",
    "    \n",
    "    X_train = data_npr[0:n_tr, 0:-1]\n",
    "    Y_train = data_npr[0:n_tr, -1]\n",
    "    \n",
    "    X_val = data_npr[n_tr:n_va, 0:-1]\n",
    "    Y_val = data_npr[n_tr:n_va, -1]\n",
    "    \n",
    "    X_test = data_npr[n_va:, 0:-1]\n",
    "    Y_test = data_npr[n_va:, -1]\n",
    "    \n",
    "    return [(X_train, Y_train), (X_val, Y_val), (X_test, Y_test)]\n",
    "\n",
    "\n",
    "# shuffle only the training data along axis 0\n",
    "def shuffle_train_data(X_train, Y_train):\n",
    "    \"\"\"called after each epoch\"\"\"\n",
    "    perm = np.random.permutation(len(Y_train))\n",
    "    Xtr_shuf = X_train[perm]\n",
    "    Ytr_shuf = Y_train[perm]\n",
    "    \n",
    "    return Xtr_shuf, Ytr_shuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 7. Implementation of required functions\n",
    "\n",
    "Complete the following function which computes the MSE value. (0.5 point) <br />\n",
    "(i.e. just a vanilla version of it.) That is, you can ignore the regularization term and also the constants $\\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(prediction, ground_truth):\n",
    "    # TODO: implement\n",
    "    D = prediction - ground_truth\n",
    "    mse = 1.0/len(ground_truth) * (D.T * D)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function which computes the prediction of your model. (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(X, W):\n",
    "    # TODO: implement\n",
    "    Yhat = X * W\n",
    "    return Yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function which computes the gradient of your loss function. (1.0 point) <br />\n",
    "*Hint: Just implementing the gradient computed in **3.** (a)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, Yhat, W, lambda_):\n",
    "    # TODO: implement\n",
    "    gradient = X.T * (Yhat - Y) + lambda_ * W\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function which performs a single update step of SGD. (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: avoid in-place modification\n",
    "def sgd(gradient, lr, cur_W):\n",
    "    # TODO: implement\n",
    "    new_W = cur_W - lr * gradient\n",
    "    return new_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following function which reformats your data as a design matrix. (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate X acc. to order of polynomial; likewise do it for W\n",
    "# where X is design matrix, W is the corresponding weight vector\n",
    "# [1 X X^2 X^3], [1 W1 W2 W3].T\n",
    "def prepare_data_matrix(X, W, order):\n",
    "    # TODO: implement\n",
    "    X_mat = np.matrix(np.column_stack((np.ones((X.shape[0], 1)), np.concatenate([np.power(X,i+1) for i in range(order)], axis=1))))\n",
    "    W_vec = None\n",
    "    if W != None:\n",
    "        W_vec = np.matrix(np.concatenate(([1], np.concatenate([W for i in range(order)])))).T\n",
    "    return X_mat, W_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 8. Training\n",
    "Complete the code in the following cell such that it performs **mini-batch gradient descent** on the training data for all possible hyperparameter combinations. (4.0 points)\n",
    "\n",
    "Note: You can also define a function, named appropriately, which performs training. But, take care to do correct bookkeeping of hyperparameter combinations, weight vectors, and the MSE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = split_data(data_npr)\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = itertools.chain(*splits)\n",
    "\n",
    "# cast to numpy matrices for easier calculation\n",
    "X_train = np.matrix(X_train)\n",
    "Y_train = np.matrix(Y_train).T\n",
    "X_val = np.matrix(X_val)\n",
    "Y_val = np.matrix(Y_val).T\n",
    "X_test = np.matrix(X_test)\n",
    "Y_test = np.matrix(Y_test).T\n",
    "\n",
    "tolerance = 1e-4\n",
    "start = 1\n",
    "\n",
    "# initialize weight vector from normal distribution\n",
    "# TODO: implement\n",
    "w_shape = X_train.shape[1]\n",
    "W_init = np.random.randn(w_shape)\n",
    "\n",
    "# cache weights for each hyperparam combination\n",
    "# TODO: implement\n",
    "weights_hist = list()\n",
    "\n",
    "# keep track of MSE for each hparam combination. will be useful for plotting\n",
    "# TODO: implement\n",
    "\n",
    "def train(X_train, Y_train, W_init, order, lr, lamb):\n",
    "    # initialize necessary stuffs\n",
    "    # TODO: implement\n",
    "    mse_hist = [np.Inf]\n",
    "    \n",
    "    nsamples = Y_train.shape[0]\n",
    "\n",
    "    # design matrix needed at this point\n",
    "    # use the function that we defined above\n",
    "    # TODO: implement\n",
    "    X_mat, W_vec = prepare_data_matrix(X_train, W_init, order)\n",
    "\n",
    "    epochs = 1\n",
    "    # goes through multiple epochs\n",
    "    while True:\n",
    "        # good idea to shuffle the train data\n",
    "        # TODO: implement\n",
    "        X_mat, Y_mat = shuffle_train_data(X_mat, Y_train)\n",
    "\n",
    "        # some more initialization\n",
    "        # TODO: implement\n",
    "        bs = batch_size\n",
    "\n",
    "        # goes through 1 epoch\n",
    "        while bs < nsamples:\n",
    "            # complete code for 1 epoch\n",
    "            # TODO: implement\n",
    "            X_batch = X_mat[(bs-batch_size):bs, :]\n",
    "            Y_batch = Y_mat[(bs-batch_size):bs, :]\n",
    "            Yhat = get_prediction(X_batch, W_vec)\n",
    "            gradient = compute_gradient(X_batch, Y_batch, Yhat, W_vec, lamb)\n",
    "            W_vec = sgd(gradient, lr, W_vec)\n",
    "            bs += batch_size\n",
    "\n",
    "\n",
    "        # after each epoch\n",
    "        # get prediction for whole X_train\n",
    "        # compute the MSE\n",
    "        # might need to do bookkeeping of mse values as well\n",
    "        prediction = get_prediction(X_mat, W_vec)\n",
    "        mse_hist.append(compute_mse(prediction, Y_mat))\n",
    "\n",
    "        # stopping/convergence criterion\n",
    "        # check whether diff-in-mse < tolerance\n",
    "        # TODO: implement\n",
    "        if abs(mse_hist[epochs - 1] - mse_hist[epochs]) < tolerance:\n",
    "            # cache weight vector for later use\n",
    "            # but we also need the hparam combination\n",
    "            # TODO: implement\n",
    "            weights_hist.append(W_vec)\n",
    "            print(\"order: {} , learning rate: {} , regularizer: {} \".format(order, lr, lamb))\n",
    "            print(\"Convergence after epoch {} with MSE {}\".format(epochs, mse_hist[epochs]), \"\\n\")\n",
    "            return W_vec, mse_hist\n",
    "        epochs += 1\n",
    "        \n",
    "\n",
    "# find optimal hyperparameters\n",
    "for order in polynomial_order:\n",
    "    for lr in learning_rates:\n",
    "        for lamb in lambdas:\n",
    "            train(X_train, Y_train, W_init, order, lr, lamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following function which selects the best hyperparameter combination (i.e. the one that gives lowest MSE on **validation data**). (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find hparams of minimum MSE on Validation data\n",
    "def find_best_hparams(weights_hist):\n",
    "    # TODO: implement\n",
    "    mse_best = np.Inf\n",
    "    for (w, weights) in enumerate(weights_hist):\n",
    "        X_val_mat, _ = prepare_data_matrix(X_val, None, hparams_comb[w][0])\n",
    "        prediction = get_prediction(X_val_mat, weights)\n",
    "        mse = compute_mse(prediction, Y_val)\n",
    "        print(mse)\n",
    "        if mse < mse_best:\n",
    "            mse_best = mse\n",
    "            hpm_best = hparams_comb[w]\n",
    "    return hpm_best, mse_best\n",
    "\n",
    "best_hpm_combination, best_mse = find_best_hparams(weights_hist)\n",
    "print(best_hpm_combination, best_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 9. Re-Training on Train + Validation data\n",
    "Complete the following function which does re-training on the combined training and validation data. (**1 point**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-run the training on X_train + X_val combined\n",
    "# Later test it on X_test; That will be our best possible MSE on test data\n",
    "# this will be more or less the same training code as you did above\n",
    "# but, here we just have only one value for each hyperparameter.\n",
    "print(*best_hpm_combination)\n",
    "# TODO: implement\n",
    "W_best, mse_hist = train(np.concatenate([X_train, X_val]), np.concatenate([Y_train, Y_val]), W_init, *best_hpm_combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the convergence of MSE values using matplotlib\n",
    "# i.e. #epochs on X-axis and MSE values on Y-axis\n",
    "# TODO: implement\n",
    "\n",
    "plt.plot(mse_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 10. Evaluation on Test set\n",
    "Evaluate your model on test data. (1.0 point)\n",
    "\n",
    "**Please note that you should keep X_test undisturbed throughout this whole phase.** Else restart the kernel and start from beginning. The whole point of this exercise would not make sense if test data has been *seen in training*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally!!!\n",
    "# test it on X_test with the Weight vector that you found above\n",
    "# this will be the generalization error of our model!!\n",
    "# TODO: implement\n",
    "\n",
    "X_test_mat, _ = prepare_data_matrix(X_test, None, best_hpm_combination[0])\n",
    "prediction = get_prediction(X_test_mat, W_best)\n",
    "mse_test = compute_mse(prediction, Y_test)[0, 0]\n",
    "\n",
    "print(\"Finally!!! MSE achieved on X_test is : {}\".format(round(mse_test, 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 11. Results\n",
    "Please report the following\n",
    "\n",
    "a) MSE value on Test data. (0.5 points)\n",
    "\n",
    "###### Answer\n",
    "\n",
    "We got an MSE of around 0.02 on the test data.\n",
    "\n",
    "b) Which hyperparameter combination turned out to be the best? In your understanding, why do you think such a combination turned out to be the best for this task? (1.0 point)\n",
    "\n",
    "##### Answer\n",
    "\n",
    "The best hyperparameter combination that turned out to be best was (5, 1e-05, 0.8). This combination may have turned out to be the best because a polynomial order of 5 does not underfit the training data too much (like a first order polynomial) but it also does not overfit too much like a 9th order polynomial would. Furthermore, a high regularization parameter is also helping not to overfit. A higher step size helps for faster convergence (unless it is so high that it overshoots)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Points**: 13.0 of 13.0\n",
    "**Comments**:\n",
    "- None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus (2 points)\n",
    "\n",
    "Now, please repeat the whole *training, validation, re-training, and testing* procedure that we talked about above with the following hyperparameter combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_order = [1]\n",
    "learning_rates = [0.1]\n",
    "lambdas = [0.1]\n",
    "\n",
    "hpm_combo = np.array([polynomial_order, learning_rates, lambdas], dtype=np.int32).flatten()\n",
    "\n",
    "W_best, mse_hist = train(np.concatenate([X_train, X_val]), np.concatenate([Y_train, Y_val]), W_init, *hpm_combo)\n",
    "\n",
    "plt.plot(mse_hist)\n",
    "\n",
    "X_test_mat, _ = prepare_data_matrix(X_test, None, hpm_combo[0])\n",
    "prediction = get_prediction(X_test_mat, W_best)\n",
    "mse_test = compute_mse(prediction, Y_test)[0, 0]\n",
    "\n",
    "print(\"Finally!!! MSE achieved on X_test is : {}\".format(round(mse_test, 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are your observations during the training phase? Please explain why such a behaviour happened."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "The loss does not converge because the learning rate is so big that it overshoots a local minimum and stochastic gradient descent starts diverging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Points**: 2.0 of 2.0\n",
    "**Comments**:\n",
    "- None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading: 20.0 of 20.0 points + 2.0 bonus points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook as the solution. The naming should include the assignment number and matriculation IDs of all members in your team in the following format:\n",
    "**assignment-4_matriculation1_matriculation2_matriculation3.ipynb** (in case of 3 members in a team). \n",
    "Make sure to keep the order matriculation1_matriculation2_matriculation3 the same for all assignments.\n",
    "\n",
    "Please submit the solution to your tutor (with **[NNIA][assignment-4]** in email subject):\n",
    "1. Maksym Andriushchenko <s8mmandr@stud.uni-saarland.de>\n",
    "2. Marius Mosbach <s9msmosb@stud.uni-saarland.de>\n",
    "3. Rajarshi Biswas <rbisw17@gmail.com>\n",
    "4. Marimuthu Kalimuthu <s8makali@stud.uni-saarland.de>\n",
    "\n",
    "Note: **If you are in a team, please submit only 1 solution to only 1 tutor.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
