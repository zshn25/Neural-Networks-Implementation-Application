{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment sheet 3: Numerical Computation and Prinicipal Component Analysis (Deadline: Nov 24, 23:59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set notebook to full width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Issues with Softmax $~$ (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture you were introduced to the softmax function which is used to generate probabilities corresponding to the output labels. Typically, the input to the softmax function is a vector of numerical values over the labels and the output is a vector(of same dimension as the input vector) of corresponding probabilities.\n",
    "**Softmax function is given by,** $~$\n",
    "$$Softmax(x)_i = \\frac{exp(x_i)}{\\sum_{j=1}^n exp(x_j)}$$\n",
    "\n",
    "**Numerical issues might occur when computing softmax functions on a computer which can perform computations\n",
    "only upto a certain precision.** [Suggested reading $-$ [chapter 4.1 of DeepLearningBook](http://www.deeplearningbook.org/contents/numerical.html)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1$. Name these numerical issues and explain them. ($1$ points)\n",
    "\n",
    "$\\frac{\\infty}{\\infty}$ occurs when the input to softmax $x_i \\xrightarrow{}\\infty $ and $x_i \\xrightarrow{} -\\infty $ i.e. when the absolute value of any input are very large. Even if each of them is not large but the sum is large due to huge size of data, then divide by zero error occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2$. Suggest a remedy (with explanation on why it works) to overcome these numerical issues occuring with Softmax computation. Prove that this remedy actually does not change the softmax criteria. Describe a situation where the proposed remedy still fails to remove instability. ($1$ point)\n",
    "\n",
    "$ x_i - max(x_i) $ brings the maximum to zero. So, this removes the problem when the large element of $x_i$ causes $ exp(x_i) \\xrightarrow{} \\infty $\n",
    "$ Softmax(x)_i = \\frac{exp(x_i - max(x_i))}{\\sum_{j=1}^n exp(x_j - max(x_i))} $\n",
    "$\\implies \\frac{\\frac{exp(x_i)}{exp(max(x_i))}}{\\frac{\\sum_{j=1}^n exp(x_j)}{exp(max(x_i))}} \\implies \\frac{exp(x_i)}{\\sum_{j=1}^n exp(x_j)} $\n",
    "\n",
    "This fails when we have very large negative values too. Then, when we take it's difference with a very large maximum value, then $ min(x_i) - max(x_i) $ blows up again. But atleast we may prevent Overflow. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3$. First write a naive Softmax implementation, in numpy, that can produce numerical instability. Then write a modified Softmax implementation which is numerically stable.  ($0.5 + 0.5 = 1$ points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.12146553  0.08107578  0.13737881  0.08524259  0.06391813  0.08492182\n",
      "  0.06455263  0.12995514  0.11848913  0.11300045]\n",
      "[ 0.12146553  0.08107578  0.13737881  0.08524259  0.06391813  0.08492182\n",
      "  0.06455263  0.12995514  0.11848913  0.11300045]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO : Define inputs\n",
    "\n",
    "def softmax_naive(inputs):\n",
    "    \"\"\"Unstable Softmax function\"\"\"\n",
    "    inputs_exp = np.exp(inputs)\n",
    "    return inputs_exp / np.sum(inputs_exp)\n",
    "\n",
    "def softmax_modified(inputs):\n",
    "    \"\"\"Stable Softmax function\"\"\"\n",
    "    \n",
    "    inputs_exp = np.exp(inputs - np.max(inputs))\n",
    "    return inputs_exp / np.sum(inputs_exp)\n",
    "\n",
    "x = np.random.rand(10);\n",
    "y1 = softmax_naive(x)\n",
    "y2 = softmax_modified(x)\n",
    "print(y1)\n",
    "print(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis $~$ (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$4$. Is PCA supervised or unsupervised, logically explain your answer. Which is the tunable parameter in PCA?\n",
    "Briefly explain the role of this parameter in PCA.  ($1+0.5+0.5 = 2$ points)\n",
    "\n",
    "PCA is unsupervised. Because the data is not labelled. PCA is just for reducing the dimensions and for lossy data compression. It shows the structure of data in lower dimensions\n",
    "The k number of dimensions to be returned as an output after compression is tunable. For example we can choose $ k \\in [1, n] $ as n then we have the same output as input. If we have k as 1 then we get the eigenvector belonging to largest eigenvlaue as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$5$. Consider the following data:\n",
    "\n",
    "setA: ${\\bf x}^{(1)}$=$(2, 4)^T$, ${\\bf x}^{(2)}$=$(2, 2)^T$, ${\\bf x}^{(3)}$=$(3, 1)^T$, ${\\bf x}^{(4)}$=$(5, 1)^T$ \n",
    "\n",
    "setB: ${\\bf x}^{(1)}$=$(-1, 1)^T$, ${\\bf x}^{(2)}$=$(-2, 2)^T$, ${\\bf x}^{(3)}$=$(-1, 3)^T$, ${\\bf x}^{(4)}$=$(-1, 4)^T$\n",
    "\n",
    "$(a)$ Compress the above sets of vectors into a one-dimensional set using PCA, i.e., derive the encoder function $f(x)=D^{T}x$ as defined in the lecture. Then apply f to the datasets inorder to compress them. ($1.5 + 1.5$ points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(b)$. For both the above sets sketch the corresponding datasets in a separate figure. \n",
    "Also include the reconstructed vectors into the corresponding figures. ($2$ points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 3 5]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJDXIcJekBhnuktQgw12SGvT/qkdqmwpY57oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1eb67f3080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1eb67c2dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "A = np.array([[2, 2, 3, 5],\n",
    "                  [4, 2, 1, 1]])\n",
    "B = np.array([[-1, -2, -1, -1],\n",
    "                  [1, 2, 3, 4]])\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(A[0], A[1], 'go')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(B[0], B[1], 'go')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent and Newton's method $~$ (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppose $f(x) = 2x^3 - 5x + 6$ **\n",
    "\n",
    "$6$. Write down the mathematical expressions for minimizing f(x) using Gradient descent(GD) and then using Newton's Method(NM). ($1$ points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$7$. Report the updated values of x, both for GD and NM, at $x = 0$. what do you observe? ($1$ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$8$. Perform GD and NM for the above function using Tensorflow. ($1.5 + 1.5$ points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# TODO : Implement Gradient Descent with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# TODO : Implement Newton's Method with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent computation and visualisation $~$ (3 + 2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now visualize the Gradient Descent algorithm to fit a straight line to data generated using  $y = \\theta_{true}x$ $~$, i.e., use this expression to first produce the data (see code below the lines starting with m=20 and following) and then try to fit a straight line to this data. Fitting a straight line means that you have to approximate this $\\theta_{true}$ parameter using the hypothesis or predictive model by minimizing the cost function defined below.\n",
    "\n",
    "**For this task you should minimize a cost function of the form:**\n",
    "$$\\frac{1}{2m}\\sum_{i=1}^m [h_{\\theta}(x^i)-y^i]^2$$\n",
    "where\n",
    "- $x^i$ is the $i^{th}$ input \n",
    "\n",
    "- $y^i$ is the true $i^{th}$ response or output\n",
    "\n",
    "- $h_{\\theta}(x)$ is the hypothesis or predictive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assume $~$ $h_{\\theta}(x) = \\theta x$ $~$ to be the hypothesis or predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate the true data which is to be fitted\n",
    "m = 20                      # number of data points for x\n",
    "theta_true = 0.5            # corresponds to the true slope\n",
    "x = np.linspace(-1,1,m)     # x values or inputs\n",
    "y = theta_true * x          # True response\n",
    "\n",
    "\n",
    "# Create a subplot window\n",
    "# On the left window plot the true data and the approximation \n",
    "# that you obtain with different estimates of the slope theta_true\n",
    "# and on the right window plot the cost function \n",
    "\n",
    "# TODO : Create the subplot window\n",
    "\n",
    "def hypothesis(x, theta):\n",
    "    \"\"\"Our \"hypothesis or predictive model\", a straight line through the origin.\"\"\"\n",
    "    \n",
    "    # TODO : Implement\n",
    "    pass\n",
    "\n",
    "def cost_func(theta):\n",
    "    \"\"\"The cost function describing the goodness of fit.\"\"\"  \n",
    "    \n",
    "    # TODO : Implement\n",
    "    pass\n",
    "\n",
    "\n",
    "# First construct a grid of theta parameter and their corresponding\n",
    "# cost function values.\n",
    "theta_grid = np.linspace(-0.2,1,50)\n",
    "# Find the cost function values to be stored in J_grid\n",
    "# TODO : Create J_grid\n",
    "\n",
    "\n",
    "# Plot the cost function as a function of theta.\n",
    "# TODO : Do the plot\n",
    "\n",
    "\n",
    "# Take N steps with learning rate alpha down the steepest gradient,\n",
    "# starting at theta = 0.\n",
    "N = 10\n",
    "alpha = 1 \n",
    "# this is just a starting value of alpha, \n",
    "# you must consider different values of alpha (try using large values)\n",
    "# and redo the steps below to generate different plots\n",
    "theta = [0]\n",
    "\n",
    "\n",
    "# TODO :Compute the N steps down the steepest gradient\n",
    "\n",
    "# TODO : Annotate the cost function plot with coloured points indicating the\n",
    "# parameters chosen and red arrows indicating the steps down the gradient.\n",
    "# Also plot the fit function on the left window of the subplot in a matching colour.\n",
    "\n",
    "# TODO : Put the labels, titles and a legend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now assume that the data is generated using  $y = \\theta_1x + \\theta_0$\n",
    "** Following the same logic you applied for the above task define a predictive model \n",
    "and perform 5 steps of gradient descent with learning rate alpha = 0.7 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the true data which is to be fitted\n",
    "m = 20\n",
    "theta0_true = 2\n",
    "theta1_true = 0.5\n",
    "x = np.linspace(-1,1,m)\n",
    "y = theta0_true + theta1_true * x\n",
    "\n",
    "# Create the sub-plot: left window is the data, right window will be the cost function.\n",
    "# TODO\n",
    "\n",
    "\n",
    "def hypothesis(x, theta0, theta1):\n",
    "    \"\"\"Our \"hypothesis function\", a straight line.\"\"\"\n",
    "    \n",
    "    # TODO : Implement\n",
    "    pass\n",
    "\n",
    "def cost_func(theta0, theta1):\n",
    "    \"\"\"The cost function, J(theta0, theta1) describing the goodness of fit.\"\"\"\n",
    "    \n",
    "    # TODO : Implement\n",
    "    pass\n",
    "\n",
    "\n",
    "# First construct a grid of (theta0, theta1) parameter pairs and their\n",
    "# corresponding cost function values.\n",
    "theta0_grid = np.linspace(-1,4,101)\n",
    "theta1_grid = np.linspace(-5,5,101)\n",
    "\n",
    "# TODO : Compute the cost function values\n",
    "\n",
    "\n",
    "# TODO : Do a labeled contour plot for the cost function on right window of the above subplot\n",
    "\n",
    "\n",
    "# TODO : Take 5 steps with learning rate alpha = 0.7 down the steepest gradient,\n",
    "# starting at (theta0, theta1) = (0, 0).\n",
    "\n",
    "\n",
    "# TODO : Annotate the cost function plot with coloured points indicating the\n",
    "# parameters chosen and red arrows indicating the steps down the gradient.\n",
    "# Also plot the fit function on the left window in a matching colour.\n",
    "\n",
    "\n",
    "# TODO : Add the labels, titles and a legend to the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra Bonus\n",
    "- [Additional material - Linear Algebra Basics](http://www.cs.ubc.ca/~schmidtm/Documents/2009_Notes_LinearAlgebra.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trace of a Matrix $~$ (3 points)\n",
    "- [Reading material on Trace](https://en.wikipedia.org/wiki/Trace_(linear_algebra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove that the trace of a ***symmetric positive definite*** matrix is the sum of its eigenvalues.    ($0.5$ points)\n",
    "\n",
    "$ tr(A) = \\sum_{i=1}^n a_{ii} $\n",
    "\n",
    "Suppose $\\mathbf{Y}$ is a $m \\times n$ matrix with $m \\leq n$ and has ***full rank***, then\n",
    "\n",
    "$(a)$.   Give the rank of $\\mathbf{Y}$.                                                                 ($0.5$ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(b)$.  Show that trace of $\\mathbf{Y}^{T}(\\mathbf{Y}^T\\mathbf{Y})^{-1}\\mathbf{Y}$ = rank($\\mathbf{Y}$)                                     ($1$ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(c)$. Prove that $\\mathbf{Y}^{T}(\\mathbf{Y}^T\\mathbf{Y})^{-1}\\mathbf{Y}$ is the projection matrix w.r.t space defined by $\\mathbf{Y}$.     ($1$ points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jacobian $~$ (3 points)\n",
    "\n",
    "***[Reading material on Jacobian](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that the Jacobian determinant of $\\frac{\\partial(fg, h)}{\\partial(u, v)}$ is equal to $\\frac{\\partial(f, h)}{\\partial(u, v)}g + f\\frac{\\partial(g, h)}{\\partial(u, v)}$,\n",
    "\n",
    "where $f$,$g$, and $h$ are functions of $u$ and $v$ (i.e., $f(u,v)$, $g(u,v)$, and $h(u,v)$)   ($3$ points)\n",
    "\n",
    "Hint: Use the property $\\frac{\\partial(y, x)}{\\partial(u, v)} = \\frac{\\partial(y)}{\\partial(u)}\\frac{\\partial(x)}{\\partial(v)}-\\frac{\\partial(y)}{\\partial(v)}\\frac{\\partial(x)}{\\partial(u)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hessian $~$ (2 points)\n",
    "***[Reading material on Hessian](https://en.wikipedia.org/wiki/Hessian_matrix)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathbf{M}=\\left[\\begin{array}{cccc}\n",
    "   5 & 1 & 0 & 1\\\\\n",
    "   1 & 4 & 1 & 0\\\\\n",
    "   0 & 1 & 3 & 1\\\\\n",
    "   1 & 0 & 1 & 2\\\\\n",
    "  \\end{array}\\right]$\n",
    "  \n",
    "denote the Hessian matrix at particular point for a particular function.\n",
    "\n",
    "$(a)$. What properties of the functional can you infer from the above information.(give mathematical reasons) ($1$ point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(b)$. Provide a generic mathematical representation (e.g. the generic representation of a straight line is $ax+by+c=0$) for the above function. ($1$ point)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
