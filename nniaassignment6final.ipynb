{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Sheet 6:  Regularization methods in Machine learning and their application in Feedforward neural networks  (deadline: 16 Dec, 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Regularization methods in ML $~$ (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Study the effects of **L2** and **L1** regularization on the weights used for modelling the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ridge regression*** is very similar to least squares, except that the weights are estimated by minimizing a slightly different quantity. In particular, the ridge regression co-efficient estimates $\\mathbf{W_{ridge}}$ are the values that minimize, \n",
    "\n",
    "$$\\mathbf{J(W) ~~=~~ \\big|\\big|~Y-XW~\\big|\\big|_{2}^2 ~+~\\lambda~ \\big|\\big|~W~\\big|\\big|_{2}^2}$$ \n",
    "\n",
    "where,\n",
    "\n",
    "$\\mathbf{\\lambda>0}$ is the regularizer,\n",
    "\n",
    "**X** is the design matrix,\n",
    "\n",
    "$\\mathbf{W}$ is the weight vector and\n",
    "\n",
    "**Y** represents the responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ridge regression*** seeks weight estimate $W^{Ridge}$ that fit the data well by minimizing the squared error $~$ $\\mathbf{||~Y-XW~||^2}$ (which was also the linear regression cost function).\n",
    "However, the second term, $\\mathbf{||~W~||^2}$, called a ***shrinkage penalty*** is small when $\\mathbf{W}$, i.e., $(w_1, w_2, ..., w_d)^T$ are close to zero. Thus, it has the effect of shrinking the estimates of $w_i$ towards zero.\n",
    "\n",
    "The regularizer $\\mathbf{\\lambda}$ serves to control the relative impact of these two terms on the regression weight estimates. when $\\mathbf{\\lambda=0}$, the penalty term has no effect, and ridge regression will produce the least squares estimates. However, as $\\mathbf{\\lambda \\rightarrow \\infty}$, the impact of the shrinkage penalty grows and the ridge regression weight estimates will approach zero. Unlike least squares, which generates only one set of weight estimates, ridge regression will produce a different set of weight estimates, $\\mathbf{W_{\\lambda}^{Ridge}}$, for each value of $\\mathbf{\\lambda}$. Hence, selecting a good value of $\\mathbf{\\lambda}$ is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1.$ **Plot the magnitude of each weight in $\\mathbf{W^{Ridge}}$ vs $\\mathbf{\\lambda}$ and explain how the regularizer $\\mathbf{\\lambda}$ affects the Ridge weights $\\mathbf{W^{Ridge}}$.** $~$ ($2.5$ points)\n",
    "\n",
    "Download the dataset, **data.csv**, from the NNIA piazza page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Read 'Salary' as your response/dependent variable\n",
    "salary = data[\"Salary\"]\n",
    "\n",
    "# Drop the column with the dependent variable 'Salary'\n",
    "data_indep = data.drop(\"Salary\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize values for the alphas\n",
    "lamdas = 10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "def cost_ridge(X, w, lamda):\n",
    "    err = Y - np.matmul(X, w)\n",
    "    return np.linalg.norm(err, 2) + lamda * np.linalg.norm(w, 2)\n",
    "\n",
    "# Create a Ridge Object that performs ridge regression\n",
    "ridge = Ridge()\n",
    "\n",
    "# Create list to hold ridge weights\n",
    "weights = []\n",
    "intercept =[]\n",
    "\n",
    "# Iterate over all lamdas, performing data fitting with ridge regression \n",
    "# and find the corresponding co-efficients\n",
    "X = data_indep.drop(\"Player\", axis=1)\n",
    "\n",
    "Y = salary\n",
    "\n",
    "for i in lamdas:\n",
    "    \n",
    "    ridge.set_params(alpha = i)\n",
    "    ridge.fit(X, Y)\n",
    "    weights.append(ridge.coef_)\n",
    "    intercept.append(cost_ridge(X, ridge.coef_, i))\n",
    "\n",
    "# Generate the plot\n",
    "ax = plt.gca()\n",
    "ax.plot(lamdas, weights)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "#plt.axis('tight')\n",
    "\n",
    "# Name the plot\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.xlim([1 1000000])\n",
    "plt.xlim([-65 65])\n",
    "plt.title('Ridge weights as a function of lambda');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "When the regularizer $\\lambda$ is very low, then the regularization term doesn't have any effect and it is same as sum of least-squares. But, as we increase $\\lambda$, we see that the weights converge. Also, $\\lambda$ shouldn't be that large as the least-squares won't have any effect then. Optimum value of $\\lambda$ would be between $10^1$ and $10^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input 'y' of 'Mul' Op has type float32 that does not match type float64 of argument 'x'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    489\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[0;32m    491\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[0;32m    740\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 741\u001b[1;33m           \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[1;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m         % (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[0;32m    615\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor conversion requested dtype float64 for Tensor with dtype float32: 'Tensor(\"Mean_2:0\", shape=(), dtype=float32)'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-76512395b05b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#for i in lamdas:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m loss = tf.add(tf.reduce_mean(tf.square(y - y_)),\n\u001b[1;32m---> 16\u001b[1;33m                              tf.multiply(lamdas, ridgeloss))\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m_mul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1375\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m   \"\"\"\n\u001b[1;32m-> 1377\u001b[1;33m   \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Mul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1378\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    524\u001b[0m                   \u001b[1;34m\"%s type %s of argument '%s'.\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\n\u001b[1;32m--> 526\u001b[1;33m                    inferred_from[input_arg.type_attr]))\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m           \u001b[0mtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Input 'y' of 'Mul' Op has type float32 that does not match type float64 of argument 'x'."
     ]
    }
   ],
   "source": [
    "# Now generate the same plot as above using Tensorflow\n",
    "# for the same set of lambdas\n",
    "\n",
    "x = tf.placeholder(\"float\", X.shape)\n",
    "y = tf.placeholder(\"float\", Y.shape)\n",
    "lambda_holder = tf.placeholder(\"float\")\n",
    "w = tf.Variable(tf.zeros(shape = [19, 1]))\n",
    "\n",
    "lamdas = 10**np.linspace(10,-2,100)*0.5\n",
    "intercept = []\n",
    "costs = []\n",
    "weights = []\n",
    "\n",
    "y_ = tf.matmul(x, w)\n",
    "#ridgeloss = tf.reduce_mean(tf.square(w))\n",
    "#for i in lamdas:\n",
    "#loss = tf.add(tf.reduce_mean(tf.square(y - y_)),\n",
    "#                             tf.multiply(tf.cast(lamdas,tf.float32), ridgeloss))\n",
    "loss = tf.reduce_mean(tf.square(y - y_) + tf.multiply(lambda_holder, tf.square(w)))\n",
    "grad = tf.train.GradientDescentOptimizer(0.001)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for alpha in lambdas:\n",
    "    train = grad.minimize(loss)\n",
    "    cost = sess.run(train, feed_dict = {x: X, y: Y, lambda_holder:alpha})\n",
    "    costs.append(cost)\n",
    "    weights.append(sess.run(w)[1][:])\n",
    "#interc = sess.run(train, feed_dict = {x: X, y: Y, lambda_holder:lambdas[0]})\n",
    "#intercept.append(interc)\n",
    "w_test = sess.run(w)[:][1]\n",
    "#print(w_test)\n",
    "#print(lamdas)\n",
    "#print(len(weights[0]))\n",
    "#print(weights)\n",
    "#for alpha in lambdas:\n",
    "\n",
    "\n",
    "# Generate the plot\n",
    "ax = plt.gca()\n",
    "ax.plot(lamdas, weights)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "\n",
    "# Name the plot\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Ridge weights as a function of lambda');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2$. Next we deal with **L1 regularization** for which the corresponding method is called **Lasso.** In Lasso, we minimize the function, \n",
    "$$\\mathbf{J(W) ~~=~~ \\big|\\big|~Y-XW~\\big|\\big|_{2}^2 ~+~\\lambda~ \\big|\\big|~W~\\big|\\big|_{1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the magnitude of each weight in $\\mathbf{W^{Lasso}}$ vs $\\mathbf{\\lambda}$ and explain how the regularizer $\\mathbf{\\lambda}$ affects the Lasso weights $\\mathbf{W^{Lasso}}$.** $~$ ($2.5$ points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEaCAYAAADQVmpMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x245fa6a3d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a Lasso Object(set max_iter to 10000)\n",
    "lasso = Lasso(max_iter = 10000)\n",
    "\n",
    "# Create list to hold lasso weights\n",
    "weights = []\n",
    "intercept = []\n",
    "\n",
    "def cost_lasso(X, w, lamda):\n",
    "    err = Y - np.matmul(X, w)\n",
    "    return np.linalg.norm(err, 2) + lamda * np.linalg.norm(w, 1)\n",
    "\n",
    "# Iterate over all alphas, performing data fitting with Lasso\n",
    "# and find the corresponding co-efficients\n",
    "for i in lamdas:\n",
    "    \n",
    "    lasso.set_params(alpha = i)\n",
    "    lasso.fit(X, Y)\n",
    "    weights.append(lasso.coef_)\n",
    "    intercept.append(cost_lasso(X, lasso.coef_, i))\n",
    "\n",
    "# Generate the plot\n",
    "\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "\n",
    "# Name the plot\n",
    "ax = plt.gca()\n",
    "ax.plot(lamdas, weights)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "\n",
    "# Name the plot\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Lasso weights as a function of lambda');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "When the regularizer $\\lambda$ is very low, then the regularization term doesn't have any effect and it is same as sum of least-squares. But, as we increase $\\lambda$, we see that the weights converge. Also, $\\lambda$ shouldn't be that large as the least-squares won't have any effect then. Optimum value of $\\lambda$ would be ~$12$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now generate the same plot as above using Tensorflow\n",
    "# for the same set of lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment on the plots generated in problems $1$ and $2$ respectively.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Impact of norms in the  Regularizer $~$ (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$4$. Assume$~$ $\\mathbf{x} \\in R^2$, $(x_1, x_2) \\in [-1, 1]\\times[-1, 1]$. $~$ (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, draw the contour plots for $\\mathbf{\\big|\\big|~x~\\big|\\big|_{0}}$, $\\mathbf{\\big|\\big|~x~\\big|\\big|_{1}}$, $\\mathbf{\\big|\\big|~x~\\big|\\big|_{2}}$ and $\\mathbf{\\big|\\big|~x~\\big|\\big|_{\\infty}}$ norms (consider all possible isolines in the given interval,i.e., ($[-1,1]\\times[-1,1]$) and **explain** how you get the corresponding plot, i.e., provide the mathematical formula for getting the outermost isoline in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://github.com/zshn25/Neural-Networks-Implementation-Application/blob/zee/img_norms.jpg?raw=true\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://github.com/zshn25/Neural-Networks-Implementation-Application/blob/zee/img_norms.jpg?raw=true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$5$. Sketch the **Lasso** optimization function, $~$ $\\mathbf{J(W) ~~=~~ \\big|\\big|~Y-XW~\\big|\\big|_{2}^2 ~+~\\lambda~ \\big|\\big|~W~\\big|\\big|_{1}}$ $~$ in two dimensions. From this sketch try to explain why **Lasso** induces **sparsity.** $~$ (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEaCAYAAADQVmpMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x245fa8a6978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.gca()\n",
    "#ax.plot(lamdas, intercept)\n",
    "\n",
    "#ax.set_xscale('log')\n",
    "#plt.axis('tight')\n",
    "\n",
    "# Name the plot\n",
    "#plt.ylabel('Lasso cost')\n",
    "#plt.title('Lasso optimization function');\n",
    "\n",
    "ax.plot(lamdas, weights)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "\n",
    "# Name the plot\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Lasso weights as a function of lambda');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the abouve plot, Lasso doesn't drive all the weights at once to zero like ridge does but drives them to zero individually. Some weights are zero faster than other. Thus, it gives sparse solution.\n",
    "\n",
    "Other intuition is that as seen in the contour image below, the Lasso can hit the corners of the constrained region, which implies that one of the coefficients is set to zero. In higher dimensions, we have many tailed star-like constrained regions. So, as Lasso hits such corners, many weights are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://github.com/zshn25/Neural-Networks-Implementation-Application/blob/zee/1513345468596-61477580.jpg?raw=true\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://github.com/zshn25/Neural-Networks-Implementation-Application/blob/zee/1513345468596-61477580.jpg?raw=true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to know Back-Propagation in details $~$ (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](https://github.com/mmarius/nnia-tutorial/blob/master/neural-net.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a **Feedforward Neural network** with one **input layer**, one **hidden layer** and one **output layer.** The **hidden layer** and **output layer** use the sigmoid function, $\\mathbf{\\sigma(x) = \\frac{1}{1+exp(-x)}}$, as **activation function.** Also note, that the network minimizes **Binary Cross Entropy loss**, given by, $$\\mathbf{J = \\sum -y\\log(\\hat{y}) - (1-y)\\log(1-\\hat{y})}$$\n",
    "\n",
    "We consider the true class labels to be **binary**, i.e., either $0$ or $1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the purpose of computing the derivatives of the loss/cost function consider the numerical values obtained by the network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input layer** consists of two nodes, $x_1$ and $x_2$ respectively. For our problem consider the following input,\n",
    "$$\\begin{bmatrix} x_1\\\\ x_2\\\\ \\end{bmatrix} = \\begin{bmatrix} -1\\\\ 1\\\\ \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden layer** is made up of 3 neurons and the corresponding matrix of weights is as given:\n",
    "$$\n",
    "\\mathbf{W_{hidden}}\n",
    "~=~\n",
    "\\begin{bmatrix} w_1^{1} & w_1^{2} & w_1^{3} \\\\ w_2^{1} & w_2^{2} & w_2^{3} \\end{bmatrix}\n",
    "~=~\n",
    "\\begin{bmatrix} 0.15 & -0.25 & 0.05\\\\ 0.20 & 0.10 & -0.15 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Note:** Output of **Hidden layer** is given by, $~~$ $\\mathbf{a=\\sigma~(W_{hidden}^{T}x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output layer** consists of one neuron, i.e., the **network** generates a single output. **For our problem, the true class label is $1$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix corresponding to the **Output layer** is given by,\n",
    "$$\n",
    "\\mathbf{W_{out}}\n",
    "~=~\n",
    "\\begin{bmatrix} w_1\\\\w_2 \\\\w_3\\end{bmatrix}\n",
    "~=~\n",
    "\\begin{bmatrix} 0.20\\\\-0.35\\\\0.15 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Note:** output from the **Output layer** is given by, $~$ $\\mathbf{\\hat{y} = \\sigma~(W_{out}^Ta)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$6$. Execute the following sequence of operations and **show that Binary Cross Entropy loss is getting reduced, i.e., $ C^2 < C^1$** $~$ ($3$ points)\n",
    "\n",
    "**Perform Forward-propagation to generate output** $\\to$ **Compute loss or cost ($C^1$)** $\\to$ **perform Back-propagation to compute the error** $\\to$ **perform Gradient descent to update the weights** $\\to$ **peform Forward-propagation again with updated weights** $\\to$ **Compute loss or cost ($C^2$)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  $C^i$ denotes the loss or cost at the $i^{th}$ iteration, for performing Gradient descent consider a learning rate of $0.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "In the hidden layer we have:\n",
    "\n",
    "$$\n",
    "Z_{in} = W_{hidden}^T x \n",
    "~=~\n",
    "\\begin{bmatrix} 0.15 & 0.20 \\\\ -0.25 & 0.10 \\\\ 0.05 & -0.15\\end{bmatrix}\n",
    "\\begin{bmatrix} -1\\\\ 1\\\\ \\end{bmatrix}\n",
    "~=~\n",
    "\\begin{bmatrix} 0.05\\\\ 0.35\\\\ -0.20 \\end{bmatrix}\n",
    "~\n",
    "\\implies\n",
    "~\n",
    "a = \\sigma(Z_{in}) ~\\approx~ \\begin{bmatrix} 0.512\\\\ 0.587\\\\ 0.450\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "In the output layer we have:\n",
    "\n",
    "$$\n",
    "Z_{out = }W_{out}^T a = \n",
    "\\begin{bmatrix} 0.20 & -0.35 & 0.15\\end{bmatrix}\n",
    "\\begin{bmatrix} 0.512\\\\ 0.587\\\\ 0.450\\end{bmatrix}\n",
    "\\approx\n",
    "-0.035\n",
    "\\implies\n",
    "\\hat{y} = \\sigma(Z_{out}) \\approx 0.491\n",
    "$$\n",
    "\n",
    "And with this result and $y = 1$ we can compute the cost of the first pass as\n",
    "\n",
    "$$\n",
    "J = \\sum -(1)\\log(0.491) - (1-1)\\log(1-0.491) \\approx 0.711 = C_1\n",
    "$$\n",
    "\n",
    "Now, we start with the backward pass. To that end, we start by computing the derivatives of the cost function w.r.t. $W_{out}$ so that we can update those weights using a gradient descent scheme:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W_{out}} = \\frac{\\partial J}{\\partial \\hat{y}} \n",
    "~\n",
    "\\frac{\\partial \\hat{y}}{\\partial Z_{out}} \n",
    "~\n",
    "\\frac{\\partial Z_{out}}{\\partial W_{out}}\n",
    "~\n",
    "=\n",
    "~\n",
    "\\left( \\frac{-y+\\hat{y}}{\\hat{y}-\\hat{y}^2} \\right)\n",
    "~\n",
    "(\\sigma(Z_{out})(1-\\sigma(Z_{out})))\n",
    "~\n",
    "a\n",
    "=\n",
    "~\n",
    "\\delta_{out}\n",
    "~\n",
    "a\n",
    "\\approx (-0.509) \\begin{bmatrix} 0.512\\\\ 0.587\\\\ 0.450\\end{bmatrix}\n",
    "\\approx \\begin{bmatrix} -0.261\\\\ -0.298\\\\ 0.229\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then we update the output weights with the gradient descent scheme\n",
    "\n",
    "$$\n",
    "W_{out} = W_{out} - \\alpha \\frac{\\partial J}{\\partial W_{out}} = \\begin{bmatrix} 0.20 \\\\ -0.35 \\\\ 0.15\\end{bmatrix} - (0.1)\\begin{bmatrix} 0.20 \\\\ -0.35 \\\\ 0.15\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 0.226 \\\\ -0.320 \\\\ 0.173\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then, we can procede to compute the derivative of the cost function $J$ w.r.t. $W_{hidden}$ in order to update them. The derivative is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W_{hidden}} = \\frac{\\partial J}{\\partial \\hat{y}} \n",
    "~\n",
    "\\frac{\\partial \\hat{y}}{\\partial Z_{out}} \n",
    "~\n",
    "\\frac{\\partial Z_{out}}{\\partial a}\n",
    "~\n",
    "\\frac{\\partial a}{\\partial Z_{in}}\n",
    "~\n",
    "\\frac{\\partial Z_{in}}{\\partial W_{in}}\n",
    "~\n",
    "=\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta_{out}\n",
    "~\n",
    "W_{out}\n",
    "~\n",
    "\\frac{\\partial Z_{out}}{\\partial a}\n",
    "~\n",
    "\\frac{\\partial a}{\\partial Z_{in}}\n",
    "~\n",
    "\\frac{\\partial Z_{in}}{\\partial W_{in}}\n",
    "~\n",
    "=\n",
    "~\n",
    "\\delta_{out}~W_{out}~\\sigma(Z_{in})(1-\\sigma(Z_{in}))(x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies\n",
    "\\delta_{in} \\frac{\\partial Z_{in}}{\\partial W_{in}}\n",
    "~\n",
    "\\approx\n",
    "~\n",
    "\\begin{bmatrix} 0.029 & -0.028 \\\\ -0.04 & 0.04 \\\\ 0.022 & -0.022\\end{bmatrix}\n",
    "\\begin{bmatrix} -1 & 1\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "With the transpose of the previous result, we can update $W_{hidden}$ with the gradient descent scheme\n",
    "\n",
    "$$\n",
    "W_{hidden} = W_{hidden} - \\alpha \\frac{\\partial J}{\\partial W_{hidden}} \n",
    "~\n",
    "=\n",
    "~\n",
    "\\begin{bmatrix} 0.15 & -0.25 & 0.05\\\\ 0.20 & 0.10 & -0.15 \\end{bmatrix}\n",
    "~\n",
    "- (0.1) \\begin{bmatrix} 0.029 & -0.04 & 0.022\\\\ -0.028 & 0.04 & -0.022 \\end{bmatrix}\n",
    "~\n",
    "=\n",
    "~\n",
    "\\begin{bmatrix} 0.147 & -0.246 & 0.047\\\\ 0.203 & 0.096 & -0.148 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And now, we compute the forward pass with the updated results. In the hidden layer we have:\n",
    "\n",
    "$$\n",
    "Z_{in} = W_{hidden}^T x \n",
    "~=~\n",
    "\\begin{bmatrix} 0.147 & -0.246 & 0.047\\\\ 0.203 & 0.096 & -0.148 \\end{bmatrix}\n",
    "\\begin{bmatrix} -1\\\\ 1\\\\ \\end{bmatrix}\n",
    "~=~\n",
    "\\begin{bmatrix} 0.056\\\\ 0.342\\\\ -0.197 \\end{bmatrix}\n",
    "~\n",
    "\\implies\n",
    "~\n",
    "a = \\sigma(Z_{in}) ~\\approx~ \\begin{bmatrix} 0.514\\\\ 0.585\\\\ 0.451\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "In the output layer we have:\n",
    "\n",
    "$$\n",
    "Z_{out = }W_{out}^T a = \n",
    "\\begin{bmatrix} 0.226 & -0.320 & 0.173\\end{bmatrix}\n",
    "\\begin{bmatrix} 0.514\\\\ 0.585\\\\ 0.451\\end{bmatrix}\n",
    "\\approx\n",
    "0.007\n",
    "\\implies\n",
    "\\hat{y} = \\sigma(Z_{out}) \\approx 0.502\n",
    "$$\n",
    "\n",
    "And with this result and $y = 1$ we can compute the cost of the first pass as\n",
    "\n",
    "$$\n",
    "J = \\sum -(1)\\log(0.491) - (1-1)\\log(1-0.491) \\approx 0.689 = C_2\n",
    "$$\n",
    "\n",
    "And from the previous computations we can see that $C_2 < C_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward Neural Network with L1 and L2 regularization $~$ (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you would build a **feed-forward network** from scratch using only **Numpy** in python. For this, you also have to implement **Back-propagation** in python. Additionally, this network should have the option of **L1 and L2 regularization** enabled within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download **mnist** dataset from NNIA piazza page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    " \n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"function for loading data\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, \n",
    "                               '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', \n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, \n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", \n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, \n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_mnist('mnist/', kind='train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 10000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = load_mnist('mnist/', kind='t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\" Feedforward neural network with a single hidden layer.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of output units, should be equal to the\n",
    "        number of unique class labels.\n",
    "        \n",
    "    n_features : int\n",
    "        Number of features (dimensions) in the target dataset.\n",
    "        Should be equal to the number of columns in the X array.\n",
    "        \n",
    "    n_hidden : int\n",
    "        Number of hidden units.\n",
    "        \n",
    "    l1 : float\n",
    "        Regularizer for L1-regularization.\n",
    "        l1=0.0 implies no regularization\n",
    "        \n",
    "    l2 : float\n",
    "        Lambda value for L2-regularization.\n",
    "        l2=0.0 implies no regularization\n",
    "        \n",
    "    epochs : int\n",
    "        Number of passes over the training set.\n",
    "        \n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate.\n",
    "        \n",
    "    decrease_const : float (default: 0.0)\n",
    "        Decrease constant. Shrinks the learning rate\n",
    "        after each epoch via eta / (1 + epoch*decrease_const)\n",
    "        \n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch if True to prevent circles.\n",
    "        \n",
    "    minibatches : int (default: 1)\n",
    "        Divides training data into k minibatches for efficiency.\n",
    "        \n",
    "    random_state : int (default: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    cost_ : list\n",
    "      Sum of squared errors after each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l1=0.0, l2=0.0, epochs=50, eta=0.001,\n",
    "                 decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, random_state=None):\n",
    "        \n",
    "        np.random.seed(random_state)\n",
    "        self.n_output = n_output\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        self.w1, self.w2 = self.initialize_weights()\n",
    "        self.cost_ = []\n",
    "\n",
    "\n",
    "    def encode_labels(self, y, k):\n",
    "        \"\"\"Encode the labels using one-hot representation\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : y represents target values.\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot array\n",
    "\n",
    "        \"\"\"\n",
    "        label = np.zeros((k, y.shape[0]))\n",
    "        for i,j in enumerate(y):\n",
    "            label[j,i] = 1.0\n",
    "        return label\n",
    "        \n",
    "        \n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"Initialize using random numbers.\"\"\"\n",
    "        \n",
    "        #w1 = np.random.uniform(-1.0, 1.0, size = [self.n_hidden, self.n_features+1])\n",
    "        #w2 = np.random.uniform(-1.0, 1.0, size = [self.n_hidden, self.n_features+1])\n",
    "        #return w1, w2\n",
    "        w1 = np.random.uniform(-1.0, 1.0,\n",
    "                               size=self.n_hidden*(self.n_features + 1))\n",
    "        w1 = w1.reshape(self.n_hidden, self.n_features + 1)\n",
    "        w2 = np.random.uniform(-1.0, 1.0,\n",
    "                               size=self.n_output*(self.n_hidden + 1))\n",
    "        w2 = w2.reshape(self.n_output, self.n_hidden + 1)\n",
    "        return w1, w2\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Compute sigmoid function\n",
    "           Implement a stable version which \n",
    "           takes care of overflow and underflow.\n",
    "        \"\"\"\n",
    "        \n",
    "        return expit(z)\n",
    "        \n",
    "        \n",
    "\n",
    "    def sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the sigmoid function\"\"\"\n",
    "        \n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "        \n",
    "        \n",
    "\n",
    "    def add_bias_unit(self, X, how='column'):\n",
    "        \"\"\"Add bias unit to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            X_new = np.ones((X.shape[0], X.shape[1]+1))\n",
    "            X_new[:, 1:] = X\n",
    "        elif how == 'row':\n",
    "            X_new = np.ones((X.shape[0]+1, X.shape[1]))\n",
    "            X_new[1:, :] = X\n",
    "        return X_new\n",
    "        \n",
    "        \n",
    "\n",
    "    def feedforward(self, X, w1, w2):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        a1 : array,\n",
    "            Input values with bias unit.\n",
    "        z2 : array,\n",
    "            Net input of hidden layer.\n",
    "        a2 : array,\n",
    "            Activation of hidden layer.\n",
    "        z3 : array,\n",
    "            Net input of output layer.\n",
    "        a3 : array,\n",
    "            Activation of output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        a1 = self.add_bias_unit(X, how='column')\n",
    "        z2 = w1.dot(a1.T)\n",
    "        a2 = self.sigmoid(z2)\n",
    "        a2 = self.add_bias_unit(a2, how='row')\n",
    "        z3 = w2.dot(a2)\n",
    "        a3 = self.sigmoid(z3)\n",
    "        return a1, z2, a2, z3, a3\n",
    "        \n",
    "        \n",
    "\n",
    "    def L2_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        return (lambda_/2.0) * (np.sum(w1[:, 1:] ** 2) +\n",
    "                                np.sum(w2[:, 1:] ** 2))\n",
    "        \n",
    "\n",
    "    def L1_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L1-regularization cost\"\"\"\n",
    "        \n",
    "        return (lambda_/2.0) * (np.abs(w1[:, 1:]).sum() +\n",
    "                                np.abs(w2[:, 1:]).sum())\n",
    "        \n",
    "        \n",
    "    def get_cost(self, y_enc, output, w1, w2):\n",
    "        \"\"\"Compute cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, one-hot encoded class labels.\n",
    "        \n",
    "        output : array, Activation of the output layer (feedforward)\n",
    "        \n",
    "        w1 : array, Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float, Regularized cost.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1 - y_enc) * np.log(1 - output)\n",
    "        cost = np.sum(term1 - term2)\n",
    "        L1_term = self.L1_reg(self.l1, w1, w2)\n",
    "        L2_term = self.L2_reg(self.l2, w1, w2)\n",
    "        cost = cost + L1_term + L2_term\n",
    "        return cost\n",
    "        \n",
    "        \n",
    "\n",
    "    def get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        a1 : array, Input values with bias unit.\n",
    "        a2 : array, Activation of hidden layer.\n",
    "        a3 : array, Activation of output layer.\n",
    "        z2 : array, Net input of hidden layer.\n",
    "        y_enc : array, one-hot encoded class labels.\n",
    "        w1 : array, Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, Gradient of the weight matrix w1.\n",
    "        grad2 : array, Gradient of the weight matrix w2.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # backpropagation\n",
    "        sigma3 = a3 - y_enc\n",
    "        z2 = self.add_bias_unit(z2, how='row')\n",
    "        sigma2 = w2.T.dot(sigma3) * self.sigmoid_gradient(z2)\n",
    "        sigma2 = sigma2[1:, :]\n",
    "        grad1 = sigma2.dot(a1)\n",
    "        grad2 = sigma3.dot(a2.T)\n",
    "\n",
    "        # regularize\n",
    "        grad1[:, 1:] += (w1[:, 1:] * (self.l1 + self.l2))\n",
    "        grad2[:, 1:] += (w2[:, 1:] * (self.l1 + self.l2))\n",
    "\n",
    "        return grad1, grad2\n",
    "        \n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input layer with original features.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, Predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        a1, z2, a2, z3, a3 = self.feedforward(X, self.w1, self.w2)\n",
    "        y_pred = np.argmax(z3, axis=0)\n",
    "        return y_pred\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input layer with original features.\n",
    "        y : array, Target class labels.\n",
    "        print_progress : bool, Prints the progress\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self.encode_labels(y, self.n_output)\n",
    "\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx = np.random.permutation(y_data.shape[0])\n",
    "                X_data, y_enc = X_data[idx], y_enc[:, idx]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                a1, z2, a2, z3, a3 = self.feedforward(X_data[idx],\n",
    "                                                       self.w1,\n",
    "                                                       self.w2)\n",
    "                cost = self.get_cost(y_enc=y_enc[:, idx],\n",
    "                                      output=a3,\n",
    "                                      w1=self.w1,\n",
    "                                      w2=self.w2)\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self.get_gradient(a1=a1, a2=a2,\n",
    "                                                  a3=a3, z2=z2,\n",
    "                                                  y_enc=y_enc[:, idx],\n",
    "                                                  w1=self.w1,\n",
    "                                                  w2=self.w2)\n",
    "\n",
    "                delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2\n",
    "                self.w1 -= delta_w1 \n",
    "                self.w2 -= delta_w2\n",
    "\n",
    "        return self\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLP(n_output=10, \n",
    "                  n_features=X_train.shape[1], \n",
    "                  n_hidden=50, \n",
    "                  l2=0.1, \n",
    "                  l1=0.0, \n",
    "                  epochs=1000, \n",
    "                  eta=0.001,\n",
    "                  decrease_const=0.00001,\n",
    "                  minibatches=50, \n",
    "                  shuffle=True,\n",
    "                  random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn.fit(X_train, y_train, print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training error for every iteration\n",
    "# in every epoch\n",
    "\n",
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "#plt.ylim([0, 2000])\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Every iteration every epoch')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training error in every epoch\n",
    "\n",
    "batches = np.array_split(range(len(nn.cost_)), 1000)\n",
    "cost_ary = np.array(nn.cost_)\n",
    "cost_avgs = [np.mean(cost_ary[i]) for i in batches]\n",
    "\n",
    "plt.plot(range(len(cost_avgs)), cost_avgs, color='red')\n",
    "plt.ylim([0, 2000])\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Training Accuracy\n",
    "y_train_pred = nn.predict(X_train)\n",
    "acc = np.sum(y_train == y_train_pred, axis=0) / X_train.shape[0]\n",
    "\n",
    "print('Training accuracy: %.2f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Test Accuracy\n",
    "y_test_pred = nn.predict(X_test)\n",
    "acc = np.sum(y_test == y_test_pred, axis=0) / X_test.shape[0]\n",
    "\n",
    "print('Test accuracy: %.2f%%' % (acc * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
