{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Sheet 6:  Regularization methods in Machine learning and their application in Feedforward neural networks  (deadline: 16 Dec, 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Regularization methods in ML $~$ (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Study the effects of **L2** and **L1** regularization on the weights used for modelling the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ridge regression*** is very similar to least squares, except that the weights are estimated by minimizing a slightly different quantity. In particular, the ridge regression co-efficient estimates $\\mathbf{W_{ridge}}$ are the values that minimize, \n",
    "\n",
    "$$\\mathbf{J(W) ~~=~~ \\big|\\big|~Y-XW~\\big|\\big|_{2}^2 ~+~\\lambda~ \\big|\\big|~W~\\big|\\big|_{2}^2}$$ \n",
    "\n",
    "where,\n",
    "\n",
    "$\\mathbf{\\lambda>0}$ is the regularizer,\n",
    "\n",
    "**X** is the design matrix,\n",
    "\n",
    "$\\mathbf{W}$ is the weight vector and\n",
    "\n",
    "**Y** represents the responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ridge regression*** seeks weight estimate $W^{Ridge}$ that fit the data well by minimizing the squared error $~$ $\\mathbf{||~Y-XW~||^2}$ (which was also the linear regression cost function).\n",
    "However, the second term, $\\mathbf{||~W~||^2}$, called a ***shrinkage penalty*** is small when $\\mathbf{W}$, i.e., $(w_1, w_2, ..., w_d)^T$ are close to zero. Thus, it has the effect of shrinking the estimates of $w_i$ towards zero.\n",
    "\n",
    "The regularizer $\\mathbf{\\lambda}$ serves to control the relative impact of these two terms on the regression weight estimates. when $\\mathbf{\\lambda=0}$, the penalty term has no effect, and ridge regression will produce the least squares estimates. However, as $\\mathbf{\\lambda \\rightarrow \\infty}$, the impact of the shrinkage penalty grows and the ridge regression weight estimates will approach zero. Unlike least squares, which generates only one set of weight estimates, ridge regression will produce a different set of weight estimates, $\\mathbf{W_{\\lambda}^{Ridge}}$, for each value of $\\mathbf{\\lambda}$. Hence, selecting a good value of $\\mathbf{\\lambda}$ is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1.$ **Plot the magnitude of each weight in $\\mathbf{W^{Ridge}}$ vs $\\mathbf{\\lambda}$ and explain how the regularizer $\\mathbf{\\lambda}$ affects the Ridge weights $\\mathbf{W^{Ridge}}$.** $~$ ($2.5$ points)\n",
    "\n",
    "Download the dataset, **data.csv**, from the NNIA piazza page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Player  AtBat   Hits  HmRun  Runs   RBI  Walks  Years  CAtBat  \\\n",
      "0    -Alan Ashby  315.0   81.0    7.0  24.0  38.0   39.0   14.0  3449.0   \n",
      "1   -Alvin Davis  479.0  130.0   18.0  66.0  72.0   76.0    3.0  1624.0   \n",
      "2  -Andre Dawson  496.0  141.0   20.0  65.0  78.0   37.0   11.0  5628.0   \n",
      "\n",
      "    CHits  CHmRun  CRuns   CRBI  CWalks  PutOuts  Assists  Errors  League_N  \\\n",
      "0   835.0    69.0  321.0  414.0   375.0    632.0     43.0    10.0         1   \n",
      "1   457.0    63.0  224.0  266.0   263.0    880.0     82.0    14.0         0   \n",
      "2  1575.0   225.0  828.0  838.0   354.0    200.0     11.0     3.0         1   \n",
      "\n",
      "   Division_W  NewLeague_N  \n",
      "0           1            1  \n",
      "1           1            0  \n",
      "2           0            1  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(263, 20)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Read 'Salary' as your response/dependent variable\n",
    "salary = data[\"Salary\"]\n",
    "\n",
    "# Drop the column with the dependent variable 'Salary'\n",
    "data_indep = data.drop(\"Salary\", axis=1)\n",
    "print(data_indep[:3])\n",
    "data_indep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEaCAYAAAA/lAFyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x263d27102e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize values for the alphas\n",
    "lamdas = 10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "# Create a Ridge Object that performs ridge regression\n",
    "# ridge = Ridge(lamdas)\n",
    "\n",
    "# Create list to hold ridge weights\n",
    "weights = []\n",
    "\n",
    "# Iterate over all lamdas, performing data fitting with ridge regression \n",
    "# and find the corresponding co-efficients\n",
    "X = data_indep.drop(\"Player\", axis=1)\n",
    "\n",
    "Y = salary\n",
    "\n",
    "for i in lamdas:\n",
    "    ridge = Ridge(alpha = i, fit_intercept=False)\n",
    "    ridge.fit(X, Y)\n",
    "    weights.append(ridge.coef_)\n",
    "\n",
    "# Generate the plot\n",
    "ax = plt.gca()\n",
    "ax.plot(lamdas, weights)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "\n",
    "# Name the plot\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Ridge weights as a function of lambda');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now generate the same plot as above using Tensorflow\n",
    "# for the same set of lambdas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2$. Next we deal with **L1 regularization** for which the corresponding method is called **Lasso.** In Lasso, we minimize the function, \n",
    "$$\\mathbf{J(W) ~~=~~ \\big|\\big|~Y-XW~\\big|\\big|_{2}^2 ~+~\\lambda~ \\big|\\big|~W~\\big|\\big|_{1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the magnitude of each weight in $\\mathbf{W^{Lasso}}$ vs $\\mathbf{\\lambda}$ and explain how the regularizer $\\mathbf{\\lambda}$ affects the Lasso weights $\\mathbf{W^{Lasso}}$.** $~$ ($2.5$ points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a Lasso Object(set max_iter to 10000)\n",
    "# TODO Implement\n",
    "\n",
    "# Create list to hold lasso weights\n",
    "# TODO Implement\n",
    "\n",
    "# Iterate over all alphas, performing data fitting with Lasso\n",
    "# and find the corresponding co-efficients\n",
    "# TODO Implement\n",
    "\n",
    "\n",
    "# Generate the plot\n",
    "# TODO Implement\n",
    "\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "\n",
    "# Name the plot\n",
    "# TODO Implement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now generate the same plot as above using Tensorflow\n",
    "# for the same set of lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment on the plots generated in problems $1$ and $2$ respectively.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Impact of norms in the  Regularizer $~$ (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$4$. Assume$~$ $\\mathbf{x} \\in R^2$, $(x_1, x_2) \\in [-1, 1]\\times[-1, 1]$. $~$ (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, draw the contour plots for $\\mathbf{\\big|\\big|~x~\\big|\\big|_{0}}$, $\\mathbf{\\big|\\big|~x~\\big|\\big|_{1}}$, $\\mathbf{\\big|\\big|~x~\\big|\\big|_{2}}$ and $\\mathbf{\\big|\\big|~x~\\big|\\big|_{\\infty}}$ norms (consider all possible isolines in the given interval,i.e., ($[-1,1]\\times[-1,1]$) and **explain** how you get the corresponding plot, i.e., provide the mathematical formula for getting the outermost isoline in each case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$5$. Sketch the **Lasso** optimization function, $~$ $\\mathbf{J(W) ~~=~~ \\big|\\big|~Y-XW~\\big|\\big|_{2}^2 ~+~\\lambda~ \\big|\\big|~W~\\big|\\big|_{1}}$ $~$ in two dimensions. From this sketch try to explain why **Lasso** induces **sparsity.** $~$ (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to know Back-Propagation in details $~$ (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](https://github.com/mmarius/nnia-tutorial/blob/master/neural-net.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a **Feedforward Neural network** with one **input layer**, one **hidden layer** and one **output layer.** The **hidden layer** and **output layer** use the sigmoid function, $\\mathbf{\\sigma(x) = \\frac{1}{1+exp(-x)}}$, as **activation function.** Also note, that the network minimizes **Binary Cross Entropy loss**, given by, $$\\mathbf{J = \\sum -y\\log(\\hat{y}) - (1-y)\\log(1-\\hat{y})}$$\n",
    "\n",
    "We consider the true class labels to be **binary**, i.e., either $0$ or $1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the purpose of computing the derivatives of the loss/cost function consider the numerical values obtained by the network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input layer** consists of two nodes, $x_1$ and $x_2$ respectively. For our problem consider the following input,\n",
    "$$\\begin{bmatrix} x_1\\\\ x_2\\\\ \\end{bmatrix} = \\begin{bmatrix} -1\\\\ 1\\\\ \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden layer** is made up of 3 neurons and the corresponding matrix of weights is as given:\n",
    "$$\n",
    "\\mathbf{W_{hidden}}\n",
    "~=~\n",
    "\\begin{bmatrix} w_1^{1} & w_1^{2} & w_1^{3} \\\\ w_2^{1} & w_2^{2} & w_2^{3} \\end{bmatrix}\n",
    "~=~\n",
    "\\begin{bmatrix} 0.15 & -0.25 & 0.05\\\\ 0.20 & 0.10 & -0.15 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Note:** Output of **Hidden layer** is given by, $~~$ $\\mathbf{a=\\sigma~(W_{hidden}^{T}x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output layer** consists of one neuron, i.e., the **network** generates a single output. **For our problem, the true class label is $1$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix corresponding to the **Output layer** is given by,\n",
    "$$\n",
    "\\mathbf{W_{out}}\n",
    "~=~\n",
    "\\begin{bmatrix} w_1\\\\w_2 \\\\w_3\\end{bmatrix}\n",
    "~=~\n",
    "\\begin{bmatrix} 0.20\\\\-0.35\\\\0.15 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Note:** output from the **Output layer** is given by, $~$ $\\mathbf{\\hat{y} = \\sigma~(W_{out}^Ta)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$6$. Execute the following sequence of operations and **show that Binary Cross Entropy loss is getting reduced, i.e., $ C^2 < C^1$** $~$ ($3$ points)\n",
    "\n",
    "**Perform Forward-propagation to generate output** $\\to$ **Compute loss or cost ($C^1$)** $\\to$ **perform Back-propagation to compute the error** $\\to$ **perform Gradient descent to update the weights** $\\to$ **peform Forward-propagation again with updated weights** $\\to$ **Compute loss or cost ($C^2$)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  $C^i$ denotes the loss or cost at the $i^{th}$ iteration, for performing Gradient descent consider a learning rate of $0.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward Neural Network with L1 and L2 regularization $~$ (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you would build a **feed-forward network** from scratch using only **Numpy** in python. For this, you also have to implement **Back-propagation** in python. Additionally, this network should have the option of **L1 and L2 regularization** enabled within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download **mnist** dataset from NNIA piazza page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    " \n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"function for loading data\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, \n",
    "                               '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', \n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, \n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", \n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, \n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = load_mnist('mnist/', kind='train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test = load_mnist('mnist/', kind='t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\" Feedforward neural network with a single hidden layer.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of output units, should be equal to the\n",
    "        number of unique class labels.\n",
    "        \n",
    "    n_features : int\n",
    "        Number of features (dimensions) in the target dataset.\n",
    "        Should be equal to the number of columns in the X array.\n",
    "        \n",
    "    n_hidden : int\n",
    "        Number of hidden units.\n",
    "        \n",
    "    l1 : float\n",
    "        Regularizer for L1-regularization.\n",
    "        l1=0.0 implies no regularization\n",
    "        \n",
    "    l2 : float\n",
    "        Lambda value for L2-regularization.\n",
    "        l2=0.0 implies no regularization\n",
    "        \n",
    "    epochs : int\n",
    "        Number of passes over the training set.\n",
    "        \n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate.\n",
    "        \n",
    "    decrease_const : float (default: 0.0)\n",
    "        Decrease constant. Shrinks the learning rate\n",
    "        after each epoch via eta / (1 + epoch*decrease_const)\n",
    "        \n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch if True to prevent circles.\n",
    "        \n",
    "    minibatches : int (default: 1)\n",
    "        Divides training data into k minibatches for efficiency.\n",
    "        \n",
    "    random_state : int (default: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    cost_ : list\n",
    "      Sum of squared errors after each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l1=0.0, l2=0.0, epochs=50, eta=0.001,\n",
    "                 decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, random_state=None):\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "\n",
    "\n",
    "    def encode_labels(self, y, k):\n",
    "        \"\"\"Encode the labels using one-hot representation\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : y represents target values.\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot array\n",
    "\n",
    "        \"\"\"\n",
    "        #TODO Implement\n",
    "        \n",
    "        \n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"Initialize using random numbers.\"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Compute sigmoid function\n",
    "           Implement a stable version which \n",
    "           takes care of overflow and underflow.\n",
    "        \"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "        \n",
    "\n",
    "    def sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the sigmoid function\"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "        \n",
    "\n",
    "    def add_bias_unit(self, X, how='column'):\n",
    "        \"\"\"Add bias unit to array at index 0\"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "        \n",
    "\n",
    "    def feedforward(self, X, w1, w2):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        a1 : array,\n",
    "            Input values with bias unit.\n",
    "        z2 : array,\n",
    "            Net input of hidden layer.\n",
    "        a2 : array,\n",
    "            Activation of hidden layer.\n",
    "        z3 : array,\n",
    "            Net input of output layer.\n",
    "        a3 : array,\n",
    "            Activation of output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement\n",
    "        \n",
    "        \n",
    "\n",
    "    def L2_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "        \n",
    "\n",
    "    def L1_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L1-regularization cost\"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "        \n",
    "    def get_cost(self, y_enc, output, w1, w2):\n",
    "        \"\"\"Compute cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, one-hot encoded class labels.\n",
    "        \n",
    "        output : array, Activation of the output layer (feedforward)\n",
    "        \n",
    "        w1 : array, Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float, Regularized cost.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "        \n",
    "\n",
    "    def get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        a1 : array, Input values with bias unit.\n",
    "        a2 : array, Activation of hidden layer.\n",
    "        a3 : array, Activation of output layer.\n",
    "        z2 : array, Net input of hidden layer.\n",
    "        y_enc : array, one-hot encoded class labels.\n",
    "        w1 : array, Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, Gradient of the weight matrix w1.\n",
    "        grad2 : array, Gradient of the weight matrix w2.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # backpropagation\n",
    "        #TODO Implement\n",
    "        \n",
    "        \n",
    "\n",
    "        # regularize\n",
    "        #TODO Implement\n",
    "        \n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input layer with original features.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, Predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input layer with original features.\n",
    "        y : array, Target class labels.\n",
    "        print_progress : bool, Prints the progress\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = MLP(n_output=10, \n",
    "                  n_features=X_train.shape[1], \n",
    "                  n_hidden=50, \n",
    "                  l2=0.1, \n",
    "                  l1=0.0, \n",
    "                  epochs=1000, \n",
    "                  eta=0.001,\n",
    "                  decrease_const=0.00001,\n",
    "                  minibatches=50, \n",
    "                  shuffle=True,\n",
    "                  random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nn.fit(X_train, y_train, print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training error for every iteration\n",
    "# in every epoch\n",
    "\n",
    "# TODO Implement\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the training error in every epoch\n",
    "# TODO Implement\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute Training Accuracy\n",
    "# TODO Implement\n",
    "\n",
    "print('Training accuracy: %.2f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute Test Accuracy\n",
    "# TODO Implement\n",
    "\n",
    "print('Test accuracy: %.2f%%' % (acc * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
