{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Sheet 6:  Regularization methods in Machine learning and their application in Feedforward neural networks  (deadline: 16 Dec, 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Regularization methods in ML $~$ (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Study the effects of **L2** and **L1** regularization on the weights used for modelling the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ridge regression*** is very similar to least squares, except that the weights are estimated by minimizing a slightly different quantity. In particular, the ridge regression co-efficient estimates $\\mathbf{W_{ridge}}$ are the values that minimize, \n",
    "\n",
    "$$\\mathbf{J(W) ~~=~~ \\big|\\big|~Y-XW~\\big|\\big|_{2}^2 ~+~\\lambda~ \\big|\\big|~W~\\big|\\big|_{2}^2}$$ \n",
    "\n",
    "where,\n",
    "\n",
    "$\\mathbf{\\lambda>0}$ is the regularizer,\n",
    "\n",
    "**X** is the design matrix,\n",
    "\n",
    "$\\mathbf{W}$ is the weight vector and\n",
    "\n",
    "**Y** represents the responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Ridge regression*** seeks weight estimate $W^{Ridge}$ that fit the data well by minimizing the squared error $~$ $\\mathbf{||~Y-XW~||^2}$ (which was also the linear regression cost function).\n",
    "However, the second term, $\\mathbf{||~W~||^2}$, called a ***shrinkage penalty*** is small when $\\mathbf{W}$, i.e., $(w_1, w_2, ..., w_d)^T$ are close to zero. Thus, it has the effect of shrinking the estimates of $w_i$ towards zero.\n",
    "\n",
    "The regularizer $\\mathbf{\\lambda}$ serves to control the relative impact of these two terms on the regression weight estimates. When $\\mathbf{\\lambda=0}$, the penalty term has no effect, and ridge regression will produce the least squares estimates. However, as $\\mathbf{\\lambda \\rightarrow \\infty}$, the impact of the shrinkage penalty grows and the ridge regression weight estimates will approach zero. Unlike least squares, which generates only one set of weight estimates, ridge regression will produce a different set of weight estimates, $\\mathbf{W_{\\lambda}^{Ridge}}$, for each value of $\\mathbf{\\lambda}$. Hence, selecting a good value of $\\mathbf{\\lambda}$ is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1.$ **Plot the magnitude of each weight in $\\mathbf{W^{Ridge}}$ vs $\\mathbf{\\lambda}$ and explain how the regularizer $\\mathbf{\\lambda}$ affects the Ridge weights $\\mathbf{W^{Ridge}}$.** $~$ ($2.5$ points)\n",
    "\n",
    "Download the dataset, **data.csv**, from the NNIA piazza page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Player  AtBat   Hits  HmRun  Runs   RBI  Walks  Years  CAtBat  CHits  \\\n",
      "0   -Alan Ashby  315.0   81.0    7.0  24.0  38.0   39.0   14.0  3449.0  835.0   \n",
      "1  -Alvin Davis  479.0  130.0   18.0  66.0  72.0   76.0    3.0  1624.0  457.0   \n",
      "\n",
      "      ...       CRuns   CRBI  CWalks  PutOuts  Assists  Errors  Salary  \\\n",
      "0     ...       321.0  414.0   375.0    632.0     43.0    10.0   475.0   \n",
      "1     ...       224.0  266.0   263.0    880.0     82.0    14.0   480.0   \n",
      "\n",
      "   League_N  Division_W  NewLeague_N  \n",
      "0         1           1            1  \n",
      "1         0           1            0  \n",
      "\n",
      "[2 rows x 21 columns]\n",
      "(263,)\n",
      "263\n",
      "   AtBat   Hits  HmRun  Runs   RBI  Walks  Years  CAtBat  CHits  CHmRun  \\\n",
      "0  315.0   81.0    7.0  24.0  38.0   39.0   14.0  3449.0  835.0    69.0   \n",
      "1  479.0  130.0   18.0  66.0  72.0   76.0    3.0  1624.0  457.0    63.0   \n",
      "\n",
      "   CRuns   CRBI  CWalks  PutOuts  Assists  Errors  Salary  League_N  \\\n",
      "0  321.0  414.0   375.0    632.0     43.0    10.0   475.0         1   \n",
      "1  224.0  266.0   263.0    880.0     82.0    14.0   480.0         0   \n",
      "\n",
      "   Division_W  NewLeague_N  \n",
      "0           1            1  \n",
      "1           1            0  \n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "# TODO Implement\n",
    "#np.genfromtxt('data.txt', delimiter=',', names=True)\n",
    "#csv = np.genfromtxt ('data.csv', delimiter=\",\",names=True)\n",
    "#csv = np.genfromtxt ('data.csv', delimiter=\",\",skip_header = 1)[:,1:]\n",
    "csv = pd.read_csv('data.csv')\n",
    "print(csv[0:2][:])\n",
    "#print(csv[:].shape)\n",
    "#print(csv[0][:])\n",
    "\n",
    "# Read 'Salary' as your response/dependent variable\n",
    "# TODO Implement\n",
    "\n",
    "# with names=True\n",
    "response = csv['Salary']\n",
    "print(response.shape)\n",
    "\n",
    "\n",
    "#salary_boolean = csv[0,:]=='Salary'\n",
    "#print(np.where(csv[0,:] not nan))\n",
    "#print(np.isnan(csv)[0:3,:])\n",
    "\n",
    "#indices = np.invert(np.isnan(csv))\n",
    "#print(indices.shape)\n",
    "#data_vals = csv[indices]\n",
    "#print(data_vals.shape)\n",
    "#print(data_vals[0:3])\n",
    "#print(salary[1:5])\n",
    "\n",
    "# Drop the column with the dependent variable 'Salary'\n",
    "#features = csv[:][1:]\n",
    "#print(features[0:2][:])\n",
    "#features = np.delete(csv,csv['Salary'],axis=1)\n",
    "features = csv.drop('Salary',axis = 1)\n",
    "features = csv.drop('Player',axis = 1)\n",
    "print(features.shape[0])\n",
    "print(features[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Player  AtBat   Hits  HmRun  Runs   RBI  Walks  Years  CAtBat  \\\n",
      "0    -Alan Ashby  315.0   81.0    7.0  24.0  38.0   39.0   14.0  3449.0   \n",
      "1   -Alvin Davis  479.0  130.0   18.0  66.0  72.0   76.0    3.0  1624.0   \n",
      "2  -Andre Dawson  496.0  141.0   20.0  65.0  78.0   37.0   11.0  5628.0   \n",
      "\n",
      "    CHits  CHmRun  CRuns   CRBI  CWalks  PutOuts  Assists  Errors  League_N  \\\n",
      "0   835.0    69.0  321.0  414.0   375.0    632.0     43.0    10.0         1   \n",
      "1   457.0    63.0  224.0  266.0   263.0    880.0     82.0    14.0         0   \n",
      "2  1575.0   225.0  828.0  838.0   354.0    200.0     11.0     3.0         1   \n",
      "\n",
      "   Division_W  NewLeague_N  \n",
      "0           1            1  \n",
      "1           1            0  \n",
      "2           0            1  \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEaCAYAAADQVmpMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f72f4552160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''# Initialize values for the alphas\n",
    "lambdas = 10**np.linspace(10,-2,100)*0.5\n",
    "#print(lambdas.shape)\n",
    "#print(lambdas[0:10])\n",
    "\n",
    "# Create a Ridge Object that performs ridge regression\n",
    "# TODO Implement\n",
    "def ridge_regression(alphas,x,y):\n",
    "    ridge_model = Ridge(alpha = alphas)\n",
    "    ridge_model.fit(x,y)\n",
    "    return ridge_model.coef_\n",
    "\n",
    "# Create list to hold ridge weights\n",
    "# TODO Implement\n",
    "weights = []\n",
    "\n",
    "# Iterate over all lambdas, performing data fitting with ridge regression \n",
    "# and find the corresponding coefficients\n",
    "\n",
    "#TODO Implement\n",
    "#for alpha in lambdas:\n",
    "#    weights.append(ridge_regression(alpha,features,response))\n",
    "#print(len(weights))\n",
    "X = features\n",
    "Y = response\n",
    "ridge = Ridge()\n",
    "for i in lambdas:\n",
    "    ridge.set_params(alpha = i)\n",
    "    ridge.fit(X, Y)\n",
    "    weights.append(ridge.coef_)\n",
    "    #intercept.append(ridge.intercept_)\n",
    "\n",
    "\n",
    "# Generate the plot\n",
    "# TODO Implement\n",
    "ax = plt.gca()\n",
    "ax.plot(lambdas, weights)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "#ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "#plt.axis('tight')\n",
    "\n",
    "#ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "\n",
    "\n",
    "# Name the plot\n",
    "# TODO Implement\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Ridge coefficients as a function of the regularization')\n",
    "#plt.axis('tight')\n",
    "#plt.show()\n",
    "'''\n",
    "# Read data\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Read 'Salary' as your response/dependent variable\n",
    "salary = data[\"Salary\"]\n",
    "\n",
    "# Drop the column with the dependent variable 'Salary'\n",
    "data_indep = data.drop(\"Salary\", axis=1)\n",
    "print(data_indep[:3])\n",
    "data_indep.shape\n",
    "\n",
    "# Initialize values for the alphas\n",
    "lamdas = 10**np.linspace(10,-2,100)*0.5\n",
    "\n",
    "# Create a Ridge Object that performs ridge regression\n",
    "ridge = Ridge()\n",
    "\n",
    "# Create list to hold ridge weights\n",
    "weights = []\n",
    "intercept =[]\n",
    "\n",
    "# Iterate over all lamdas, performing data fitting with ridge regression \n",
    "# and find the corresponding co-efficients\n",
    "X = data_indep.drop(\"Player\", axis=1)\n",
    "\n",
    "Y = salary\n",
    "\n",
    "for i in lamdas:\n",
    "    \n",
    "    ridge.set_params(alpha = i)\n",
    "    ridge.fit(X, Y)\n",
    "    weights.append(ridge.coef_)\n",
    "    intercept.append(ridge.intercept_)\n",
    "\n",
    "# Generate the plot\n",
    "ax = plt.gca()\n",
    "ax.plot(lamdas, weights)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "#plt.axis('tight')\n",
    "\n",
    "# Name the plot\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Ridge weights as a function of lambda');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now generate the same plot as above using Tensorflow\n",
    "# for the same set of lambdas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$2$. Next we deal with **L1 regularization** for which the corresponding method is called **Lasso.** In Lasso, we minimize the function, \n",
    "$$\\mathbf{J(W) ~~=~~ \\big|\\big|~Y-XW~\\big|\\big|_{2}^2 ~+~\\lambda~ \\big|\\big|~W~\\big|\\big|_{1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the magnitude of each weight in $\\mathbf{W^{Lasso}}$ vs $\\mathbf{\\lambda}$ and explain how the regularizer $\\mathbf{\\lambda}$ affects the Lasso weights $\\mathbf{W^{Lasso}}$.** $~$ ($2.5$ points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Lasso coefficients as a function of the regularization')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XmcZHV97//Xu6p6nY3ZWIaZYQiM\njIAj4oh75HfFK6KieFEBMSIo15sYc39u0VxjiFkw8ZfkpwYXYlzZAoo4kuFyiVFxYZlRBMMWBgRn\nhZlhemZ6eq2qz/3jnG5qiq7u6u6qrq7u9/PxqEdVnfVz6pw6n/P9fs+iiMDMzAwg0+gAzMxs+nBS\nMDOzYU4KZmY2zEnBzMyGOSmYmdkwJwUzMxvmpDBDSDpC0u2SDkj6OyW+JmmvpLslvVLSw1VM5x2S\n/s9UxDzdSDpB0j3pb/iBKZzvSkndkrJTNc90vodsM1WO87ikM+odW61JukjSTycx/p9I+kotY0qn\n+yVJf1rr6U5GrtEB1JOkx4H3RMS/NTqWKXApsBuYHxEh6ZXAa4DlEXEwHeaEsSYSEVcDV9ciIEkB\nrI6IzbWY3hT4KPCjiHhBPWdSvl1GxG+BufWcZwWHbDPlPSV9HdgaEZ+Y6sCmm4j468lOQ9JFJOv9\nFSXTfd9kp1trLinMHMcAD5T8uY8BHi9JCDa2Y4D7Gx3EFCrfZhpG0rQ9QJ3OsdVFRMzYF/A4cMYI\n3RcCNwO7gL3p5+Ul/S8CHgMOAL8B3pF2Px74MbCP5AjrX0rGeRmwMe23EXjZKHGtAG5M578H+Me0\newb4BPAE8BTwTWBByXgvAX4OdAH3Aqen3b8ODAIDQDfw34E+oJB+/3PgdJKjvrFiuAj4aclwa4Db\ngKeBh4G3lfT7OnAF8K/pb3UXcFza73YggINpDG8HlqS/dVc6vZ8AmQq/0WeBLcB+4BfAK0v6nQZs\nSvs9Cfx9hWmMup7Lhv339PfqS+N9DvAjkiM7Kvw2AbwPeCSd/hWASvq/F3gw/W0eAE4FvgUUgd50\nPh8FVqXTyqXjLQPWp7/RZuC9JdO8DLg+3TYOkCSxdaNsayNulyNsM2eUjXdpWf/vl/ynPgzcl07z\nX4D2kvHeAPwqXcc/B9aOElsAf5D+fr+pYntbDHw/Xe8bgb8cWh/lv2HabXj9jbDuRtu+LgO+DVyV\n9n9P2u2qtP8/pr/J0CsPXJb2+xjwaMk6Pyft/lwO/U92layHvyzbZjany78eWFbt9laz/WatJzid\nXlROCouB/wZ0AvOAG4Cb0n5z0g3hhPT7UcBJ6edrgf9FsvNuB16Rdl+UrqR3klTJnZ9+XzzCvLMk\nO/R/SOdVOp2L0w3id0iqE24EvpX2O5pk531WOv/XpN+XVti4yv8Ep5MmhTFiGB4v7bcFeHe6XKeS\nJMOTSub5NMlOOkdS7XRd2UZ8fMn3y4EvAS3p65WVNmrgwnQ95YAPATtJdz7AHcA7089zgZdUmEbF\n9Vxh+B9xaBIo/17+mwZJojkMWEmSfM5M+70V2Aa8CBDJAcUxI22XPDsp/Bj4QrpeTkmn++q032Uk\nO5ez0vV4OXBnheUZdbss32ZGGP9Z/dPY7yZJXItIkt770n6nkhzMvDiN7V3p8G0Vph8kCWAR0MHY\n29t16asTODEddqJJYbTt6zKShPhmkv9aByVJoWwZhtbPC0rW+7J0vLeTHBQdNVIM5b8x8F/S5T0V\naAM+D9xezfZWy9esrD6KiD0R8Z2I6ImIA8BfAa8qGaQInCypIyJ2RMRQlcIgSZF7WUT0RcRQw9Xr\ngUci4lsRkY+Ia4GHgDeOMPvTSDaaj0TEwbLpvIPkqPexiOgGPg6clxZfLwQ2RMSGiChGxG0kR8tn\nTeAnGC2GUm8gqYL6WrpcvwS+A5xbMsyNEXF3RORJksIpo8x3kCTJHhMRgxHxk0i39nIRcVW6nvIR\n8Xckf5ITSqZzvKQlEdEdEXdWmMZY67kWPh0RXZG0C/yQZ5b/PcDfRsTGSGyOiCfGmpikFcArgD9O\n18uvgK+Q7NiH/DTdDgokJY/nV5jceLbL8fhcRGyPiKdJjtyHlvm9wJcj4q6IKETEN4B+khJuJZdH\nxNMR0cso21vaCP/fgD9L1+cDwDcmugBjbF8Ad0TETel/rXekaUhaCtwE/GFE3JNO94b0tylGxL+Q\nHNWfVmVY7wC+GhG/jIh+kv//SyWtKhmm0vZWM7MyKUjqlPRlSU9I2k9S1XGYpGwkdfBvJymm7ZD0\nr5LWpKN+lOSo725J90u6OO2+jKTKp9QTJEf35VYAT6Q70XLl03mC5EjmCJJk9FZJXUMvkp3HUeNc\n/LFiKHUM8OKyeb4DOLJkmJ0ln3sYvcH0MyQlof8j6TFJH6s0oKQPSXpQ0r50vgtIqp8ALiGp3nlI\n0kZJb6gwjYrrefTFHpdKy7+CpBphvJYBT6dJbEj5tlQ+z/YK9d7j2S7Ho9IyHwN8qGx7WZHGUcmW\nks+jbW9LSf4LWyqMOy5jbF9jTltSC0kV0zURcV1J99+T9KuS+E8um+5oDllf6YHhHkZf9zU/QWF2\nNaA840MkRwUvjoidkk4B7iHZ4RMRtwK3Suogqbf8J5I6x50kR0NIegXwb5JuB7aTbNClVgL/e4R5\nbwFWSsqNsFMun85KkvrKJ9PxvhUR753gMlcbQ/lwP46I19RgnqQ7ug+R7DhOAn4oaWNE/KB0uPTM\nqT8GXg3cHxFFSXt5Zv08ApwvKQO8Bfi2pMXx7Eb1UddzFQ6SVFUMObLSgCPYAhxXod9oDbvbgUWS\n5pUkhpUkVVHjNZ7tciTjbYDeAvxVRPzVOMYpnUfF7S1N5HlgOfCfaecVJYMMrftOkupfqLC+xtq+\nRohrJJ8naTcYPjNL0jEk+4pXk5Q0CpJ+VTLdsaZ5yPqSNIekimsi637CZkNJoUVSe8krR1K/3At0\nSVoE/NnQwOm522enK6SfpFGokPZ7q6Tl6aB7SVZyAdgAPEfSBZJykt5OUud58wjx3A3sAD4taU4a\n08vTftcC/6+kYyXNBf6apDE7T9Lo9UZJr5WUTcc7vSSe8RgthlI3p8v1Tkkt6etFkp5b5XyeJGkf\nAUDSGyQdL0kkf9xC+io3j2QHsAvISfokML9kOhdKWhoRRZIGTUaZzojruUq/At6SljiOJymhVOsr\nwIclvVCJ49OdBpT9LqUiYgtJA+3l6XpZm853IqcJj2e7HEnFOCv4J+B9kl6cLvMcSa+XNK/K8Stu\nb2lV2Y3AZen6WAP83tCIEbGLZOd5Yfr/uJjKSXnU7Wsskv47STXkBek2OGQOyT5hVzrcu0lKCkOe\nBJZLaq0w6WuAd0s6RVIbyf//roh4vNrYamE2JIUNJDuGoddlwP9P0ni0G7iTQ4+cMiRHmNtJGlFf\nBfx+2u9FwF2SuknODPijiPhNROwhqQ/9EElx76PAGyJid3kw6cb9RpKGx98CW0mqqwC+SlJHfDvJ\nWU99wB+m420B3gT8CclGtwX4CBNYh2PEUDrcAeC/Auelv8dO4G9I6l+rcRnwjbQo/TZgNfBvJIn2\nDuALEfGjEca7FbiF5IjwCZLfobQ4fyZwf7oePgucFxF9I0xntPVcjX8gOfvmSZL666p3zBFxA0kb\nxjUkR5Q3kTSoQtI4/In0d/nwCKOfT9Jwuh34Lkk9+m3jjJ3xbJcV/DNwYhrnTVXMbxNJSfofSQ6a\nNpM0rlYb71jb2/tJqnl2kvxPriU5cBvyXpL/xB7gJJLkOpKxtq+xnE+SLLcrueiwW9KfpO0cf0ey\nbT8JPA/4Wcl4/05ytthOSSPtG34A/ClJO8oOkqR23jjiqglVaOczM5vWJP0NcGREvKvRscwks6Gk\nYGYzgKQ1ktamVVOnkVSrfbfRcc00s7Wh2cyazzySKqNlJNdD/B3wvYZGNAO5+sjMzIa5+sjMzIY5\nKZiZ2bCma1NYsmRJrFq1qtFhmJk1lV/84he7I2LpWMM1XVJYtWoVmzZtanQYZmZNRdKY994CVx+Z\nmVkJJwUzMxvmpGBmZsOcFMzMbFjdkoKkr0p6StJ/VOgvSZ+TtFnSfZJOrVcsZmZWnXqWFL5OcjfL\nSl5HctfM1STPg/1iHWMxM7Mq1O2U1Ii4XYc+Rq7cm4Bvpo9jvFPSYZKOiogd9YrJbKaLYkAhiEKR\nKKSfi8XkPYBiQDGS4dLvEenn4XeAdPihRwMPf06GK705TvkTi4afJkw6v5GIQx85kxHKCmUzkNWh\nEy4dLiJ5WO5wR6XTGoq3wvxihM9Dy1HyfcThp5GWI+eQW9Re13k08jqFozn0HuZb027PSgqSLiUp\nTbBy5copCc6sEYoDBYr7ByjsH6BwYIBizyDFg4MUe/IU+/IU+wpEX57iQIEYKBADRWKwSOST94o7\nYZsRDnvz8cx9yUSewFu9RiaFkR6JWOkh7lcCVwKsW7fOW701tYig0NXP4I6DDO48yOBTPRSe7iP/\ndB/F7sERx1F7lkx7jkx7DrVnyc5pQQvbUUsmeeWeeSebSY+6NfyZjFBGSYVxRkgClX2H5F+p9EM6\nCOiZimaVHcUPL9SzuyXzqPQjlAwvJSWWQpLcKDJ81D482CHxqaSkkZZiMnom1krzLO1ethyqtFzl\nw4+mvJRSzTjjlF1Q6aFttdPIpLCVQ5+xupzkaUtmM0pEkH+yh75H9tL/+H4GHt9P8eAzO//sYW3k\nFrfTceJisgvbyM5vIzu/ley8VjJzWsh05pJqFbMp0MiksB54v6TrgBcD+9yeYDNFRDC4tZuee3fR\n+8AeCk8nTwvNLmqn/YSFtK6cT8tRc2g5spNMW9PdbcZmsLptjZKuBU4HlkjaSvLQ9BaAiPgSybOT\nzyJ5jmsP8O56xWI2VYoDBXrv3UX3nTsY3NYNOdF+3GHMe9VyOtYsIrug2sdbmzVGPc8+On+M/gH8\nQb3mbzaVolDk4Mad7P+331LsHiR3RCeHvfk4Ok85nEy7SwLWPLy1mk1S7wN72LfhN+R399K6aj4L\nLlhD67ELnmm8NGsiTgpmE1QcKNC1/lF6Nj1J7vBOFr/rRNrXLHIysKbmpGA2AQPbunn62ofI7+ll\n3unLmX/GMcnpoGZNzknBbJz6Hn6aPVc9SKYjx5L3PI/24w5rdEhmNeOkYDYOPb/ezdPXPUTL4Z0s\nueRksnPrfzGR2VRyUjCr0sFfPsneG/6T1hXzWPLuk8l0+O9jM4+3arMq9D2yl703/Cdtxx3G4nee\nSKYt2+iQzOrCLWNmY8g/3cfT1z6UnGHkhGAznJOC2SiKAwX2fOsBoghLnBBsFnBSMKsgIth74yMM\n7jzIovNPILeko9EhmdWdk4JZBX0P7KH3V7uYf8YxdJywqNHhmE0JJwWzERQHCnR9/zFajuxk3ukr\nxh7BbIZwUjAbwYF/30Khq5/D3nx88pAas1nCScGszOBTPRz4yVY6Tz2ctlULGh2O2ZRyUjArERF0\nrX8UtWRY8LpjGx2O2ZRzUjAr0b+5i/7NXSx4zTFk5/kWFjb7OCmYlThw+1Yyc1uYc9pRjQ7FrCGc\nFMxSA9u76X+ki7kvX4Za/New2clbvlmq+yfbUGuGuS92KcFmLycFMyDf1U/PvbuY86IjyXS2NDoc\ns4ZxUjADun+6DQjmvuLoRodi1lBOCjbrFfvyHLx7Jx1rl5Jb2N7ocMwayknBZr3eX+8mBgrMfdmy\nRodi1nBOCjbr9dy7i+zidlpXzGt0KGYN56Rgs1rhwAD9j3bRuXYpku9xZOakYLNa7693Q0DnKUsb\nHYrZtOCkYLNaz727yB3RScsRcxoditm04KRgs1a+q4+BJ/a7lGBWwknBZq3ee3cD0LnWScFsiJOC\nzVo99z5Fy/K55Bb72ctmQ+qaFCSdKelhSZslfWyE/isl/VDSPZLuk3RWPeMxGzK4q4fB7QfpfP7h\njQ7FbFqpW1KQlAWuAF4HnAicL+nEssE+AVwfES8AzgO+UK94zEr1/edeADpOWtzgSMyml3qWFE4D\nNkfEYxExAFwHvKlsmADmp58XANvrGI/ZsP7NXWQXtZNb5NtamJWqZ1I4GthS8n1r2q3UZcCFkrYC\nG4A/HGlCki6VtEnSpl27dtUjVptFohD0P7aP9uMPa3QoZtNOPZPCSJeHRtn384GvR8Ry4CzgW5Ke\nFVNEXBkR6yJi3dKlPlPEJmdg2wGiv0DbcU4KZuXqmRS2AitKvi/n2dVDlwDXA0TEHUA7sKSOMZnR\n/2gXAG3HLWhwJGbTTz2TwkZgtaRjJbWSNCSvLxvmt8CrASQ9lyQpuH7I6qp/cxctR84hO7e10aGY\nTTt1SwoRkQfeD9wKPEhyltH9kj4l6ex0sA8B75V0L3AtcFFElFcxmdVMDBbof2I/bW5PMBtRrp4T\nj4gNJA3Ipd0+WfL5AeDl9YzBrFT/E/shH04KZhX4imabVfo374OMaDt2/tgDm81CTgo2q/Q92kXr\ninlk2upaSDZrWk4KNmsUe/MMbj3gs47MRuGkYLNG/2/2QeCL1sxG4aRgs8bA1gMgaFnuZzGbVeKk\nYLPG4PaD5A7vJNOabXQoZtOWk4LNGgPbDtB69NxGh2E2rTkp2KxQ2N9P8cAgLU4KZqNyUrBZYWBb\nN4BLCmZjcFKwWWFwW3fSyHyUk4LZaJwUbFYY2NZNbmkHmTY3MpuNxknBZoXBbd20Hu1TUc3G4qRg\nM17hwACF/QO0LHPVkdlYnBRsxnMjs1n1nBRsxhtMk0LL0XMaHInZ9OekYDPewLZucks6fGdUsyo4\nKdiMN7it2xetmVXJScFmtEL3AIV9/W5PMKuSk4LNaIPbDwK4pGBWJScFm9EGdyRJofUoNzKbVcNJ\nwWa0/O5eMnNbyHS2NDoUs6YwZlKQ9FZJ89LPn5B0o6RT6x+a2eQN7uoht6Sj0WGYNY1qSgp/GhEH\nJL0CeC3wDeCL9Q3LrDbyu3udFMzGoZqkUEjfXw98MSK+B7TWLySz2ij25Sl2D9Ky1EnBrFrVJIVt\nkr4MvA3YIKmtyvHMGiq/qxfAJQWzcahm5/424FbgzIjoAhYBH6lrVGY1kN+dJoWlnQ2OxKx5VJMU\nvhwRN0bEIwARsQN4Z33DMpu8wd29IMgtam90KGZNo5qkcFLpF0lZ4IX1CcesdvK7esgubEc513aa\nVaviv0XSxyUdANZK2p++DgBPAd+bsgjNJii/u9eNzGbjVDEpRMTlETEP+ExEzE9f8yJicUR8fApj\nNBu3iPDpqGYTMGa5OiI+LuloSS+T9LtDr2omLulMSQ9L2izpYxWGeZukByTdL+ma8S6A2UiK+weI\ngaKTgtk4jXmDeUmfBs4DHuCZaxYCuH2M8bLAFcBrgK3ARknrI+KBkmFWAx8HXh4ReyUdPqGlMCsz\nOHzmkZOC2XhU89SRc4ATIqJ/nNM+DdgcEY8BSLoOeBNJchnyXuCKiNgLEBFPjXMeZiN65hoFn45q\nNh7VnJbxGDCRu4kdDWwp+b417VbqOcBzJP1M0p2SzhxpQpIulbRJ0qZdu3ZNIBSbbfK7e1FLhux8\nX3xvNh4VSwqSPk9STdQD/ErSD4Dh0kJEfGCMaWuEbjHC/FcDpwPLgZ9IOjm9SO6ZkSKuBK4EWLdu\nXfk0zJ5lqJFZmZE2QzOrZLTqo03p+y+A9ROY9lZgRcn35cD2EYa5MyIGgd9IepgkSWycwPzMhuV3\n9dCyzA/WMRuvikkhIr4xyWlvBFZLOhbYRtJYfUHZMDcB5wNfl7SEpDrpsUnO12a5yBfJ7+2jY+3S\nRodi1nSqOfvo1zy72mcfSUniLyNiz0jjRURe0vtJ7puUBb4aEfdL+hSwKSLWp/3+q6ShM5s+Uml6\nZtXK7+2Dos88MpuIas4+uoVkhz10DcF5JO0F+4CvA2+sNGJEbAA2lHX7ZMnnAD6YvsxqwndHNZu4\napLCyyPi5SXffy3pZxHxckkX1isws4kaujtqi5OC2bhVc0rqXEkvHvoi6TRgqAUvX5eozCYhv7cP\ntWX9XGazCaimpPAe4KuS5pJUG+0H3iNpDnB5PYMzm4jCvgGyh7U1OgyzpjRmUoiIjcDzJC0AVHYN\nwfV1i8xsggr7+skucFIwm4jRLl67MCKukvTBsu4ARMTf1zk2swkpdPXTerSvUTCbiNFKCnPS93lT\nEYhZLcRgkeLBQZcUzCZotIvXvpy+//nUhWM2OYV9yZ1YnBTMJmbMs48kPUfSDyT9R/p9raRP1D80\ns/HLDyWFw3wjPLOJqOaU1H8ieebBIEBE3EdyAZvZtFPocknBbDKqSQqdEXF3WTdfn2DTkquPzCan\nmqSwW9JxpPc/knQusKOuUZlNUGFfP5nOHJnWbKNDMWtK1Vy89gckzzJYI2kb8BvgHXWNymyCCl2+\nRsFsMqq5eO0x4Iz0CuZMRByof1hmE1PY1092YXujwzBrWtWcffSopKuBd3LoQ3PMpp1814BLCmaT\nUE2bwonAl4HFwP8n6TFJ361vWGbjV+wvEH153/fIbBKqSQoFktNRC0AReBJ4qp5BmU3E0JlHOZcU\nzCasmobm/cCvgb8H/slPRrPpytcomE1eNSWF84Hbgd8HrpP055JeXd+wzMZv+BoFVx+ZTVg1Zx99\nD/iepDXA64D/CXwU8GOtbFrJd/WDIDvft7gwm6hqzj76jqRHgc+S3Dn194CF9Q7MbLwK+/rJzG1B\nuWoKwGY2kmraFD4N/DIiCvUOxmwy/HAds8kb85AqIjY6IVgzKOzr95lHZpPkcrbNCBFBocvPZjab\nLCcFmxGir0AMFFx9ZDZJ1bQpIOls4HfTrz+OiO/XLySz8fPpqGa1Uc3ZR5cDfwQ8kL4+kHYzmzby\nvnDNrCaqKSm8HjglIooAkr4B3EPyNDazacElBbPaqLZN4bCSzwvqEYjZZBS6+iED2Xm+cM1sMqop\nKVwO3CPph4BI2hZcSrBppXBggMzcVpRRo0Mxa2rV3ObiWkk/Al5EkhT+OCJ21jsws/EoHhwkO6el\n0WGYNb1qGppfDuyPiPXAPOCjko6pZuKSzpT0sKTNkj42ynDnSgpJ66qO3KxEsXuQzFwnBbPJqqZN\n4YtAj6TnAx8BngC+OdZIkrLAFSQ30TsROF/SiSMMNw/4AHDXOOI2O0Th4CDZuW5PMJusapJCPiIC\neBPwuYj4LEmJYSynAZsj4rGIGACuS6dR7i+AvwX6qozZ7FmK3QNkXH1kNmnVJIUDkj4OXAj8a1oC\nqObfdzSwpeT71rTbMEkvAFZExM2jTUjSpZI2Sdq0a9euKmZts0lxoEAMFF19ZFYD1SSFtwP9wCVp\nA/PRwGeqGG+k00BiuKeUAf4B+NBYE4qIKyNiXUSsW7p0aRWzttmk2D0I4IZmsxqo5pTUA8BnI6Ig\n6TnAGuDaKsbbCqwo+b4c2F7yfR5wMvAjSQBHAuslnR0Rm6oJ3gySM48AlxTMaqCaksLtQJuko4Ef\nAO8Gvl7FeBuB1ZKOldQKnAesH+oZEfsiYklErIqIVcCdgBOCjVuhewDADc1mNVBNUlBE9ABvAT4f\nEecAJ401UkTkgfcDtwIPAtdHxP2SPpXeYM+sJoaqj9zQbDZ51VQfSdJLgXcAl6TdstVMPCI2ABvK\nun2ywrCnVzNNs3IFVx+Z1Uw1JYU/IrmtxXfTI/3fAX5Y37DMqlfsHkStGTKtVR2rmNkoqrnNxe0k\n7QpD3x8judjMbFoodif3PTKzyRszKUhaCnyUpB2hfah7RPyXOsZlVrWC73tkVjPVVB9dDTwEHAv8\nOfA4yZlFZtOC73tkVjvVJIXFEfHPwGBE/DgiLgZeUue4zKpWODjoM4/MaqSas48G0/cdkl5PcgHa\n8vqFZFa9iKDY7ZvhmdVKNUnhLyUtILkdxeeB+cD/rGtUZlWK3jwUw9VHZjVSzdlHQzer2wf8PwCS\nnBRsWhi6RiHrpGBWE9U+o7ncB2sahdkE+Wpms9qaaFLwg3BtWigMJQW3KZjVxESTQow9iFn9FQ8O\n3QzPJQWzWqjYpiDpACPv/AV01C0is3EYrj7qdFIwq4WKSSEiqnnkpllDFboHyXTmUNY1mma1MNHq\nI7NpoegL18xqyknBmlqhe8DXKJjVkJOCNTVfzWxWW04K1tRcfWRWW04K1rSiUKTYk/fpqGY15KRg\nTat4MA/4MZxmteSkYE2r0J1cuJaZ4zYFs1pxUrCmVfTN8MxqzknBmtbw1cxOCmY146RgTWv4ttk+\n+8isZpwUrGkVuwchI9RRzbOizKwaTgrWtArdA2TmtCD5vkdmteKkYE2r2JMn0+lSglktOSlY04q+\nPBlXHZnVlJOCNa1ir5OCWa05KVjTclIwqz0nBWtaTgpmtVfXpCDpTEkPS9os6WMj9P+gpAck3Sfp\nB5KOqWc8NnNEMYj+Amp3UjCrpbolBUlZ4ArgdcCJwPmSTiwb7B5gXUSsBb4N/G294rGZJfrSm+G5\npGBWU/UsKZwGbI6IxyJiALgOeFPpABHxw4joSb/eCSyvYzw2gxR7nRTM6qGeSeFoYEvJ961pt0ou\nAW4ZqYekSyVtkrRp165dNQzRmpWTgll91DMpjHSZaYw4oHQhsA74zEj9I+LKiFgXEeuWLl1awxCt\nWTkpmNVHPf9RW4EVJd+XA9vLB5J0BvC/gFdFRH8d47EZxEnBrD7qWVLYCKyWdKykVuA8YH3pAJJe\nAHwZODsinqpjLDbDDCUF3wzPrLbqlhQiIg+8H7gVeBC4PiLul/QpSWeng30GmAvcIOlXktZXmJzZ\nIXz2kVl91PUfFREbgA1l3T5Z8vmMes7fZq5ibz65bXaLr780qyX/o6wpDV3N7Ntmm9WWk4I1Jd/i\nwqw+nBSsKRV7825kNqsDJwVrSsW+gksKZnXgpGBNKVx9ZFYXTgrWlIq9g2Tas40Ow2zGcVKwphMR\nFHsLZDpaGh2K2YzjpGBNJwaKUAxXH5nVgZOCNZ1nbnHh6iOzWnNSsKbjW1yY1Y+TgjWdYo+Tglm9\nOClY0ykOlRT8fGazmnNSsKbjZymY1Y+TgjUdJwWz+nFSsKYzfPaRq4/Mas5JwZpO9OZRexZlfNts\ns1pzUrCm49tmm9WPk4I1nWJf3mcemdWJk4I1jf7+fm677Tae3rHbz1IwqxP/s6wpPPLII9x8883s\n27ePI/pfzJ5WOLzRQZnNQC4uvCk0AAALzklEQVQp2LSWz+f53ve+x9VXX01LSwsXX3wxc1s62b5n\nJ3fddVejwzObcVxSsGnr4MGDXHfddWzZsoVXvvKVvOpVryKXy7EttjBn0TxuueUW5syZw8knn9zo\nUM1mDCcFm5Z27drFNddcw4EDBzj33HOHd/yRLxKDRU5Y+1zu/+0OvvOd79DT08Npp53W4IjNZgZX\nH9m0s3v3br72ta8xMDDARRdddEhJYOi+Ry1z27jwwgtZvXo1GzZsYMOGDRQKhUaFbDZjOCnYtHLg\nwAGuuuoqAC6++GKWL19+SP/SW1y0tbVx3nnn8dKXvpS7776bq6++mq6urimP2WwmcVKwaaO/v59r\nrrmGgwcPcsEFF7B48eJnDVN+i4tMJsNrX/tazj77bLZs2cIXvvAF7rrrLorF4pTGbjZTuE3BpoVi\nscgNN9zAzp07ueCCC55VQhgSFW6Gd+qpp3Lsscdy8803c8stt3Dfffdx+umnc/zxxyP5dhhm1XJJ\nwaaFjRs3snnzZs466yxWr15dcbjR7pC6cOFCLrzwQs455xwOHDjA1VdfzZVXXskDDzzg9gazKrmk\nYA23Z88ebrvtNlavXs26detGHbY4xqM4JfH85z+fk046iXvvvZef/vSnXH/99XR2dvK85z2PtWvX\nsmzZMpcezCpwUrCGKhaL3HTTTeRyOd74xjeOubOu9lkKuVyOF77whZxyyik88sgj3HfffWzatIm7\n7rqLOXPmcNxxx3HcccexYsUKFi5c6CRhlqprUpB0JvBZIAt8JSI+Xda/Dfgm8EJgD/D2iHi8njHZ\n9HLHHXewZcsWzjnnHObPnz/m8MXePGrJoFx1NZ/ZbJY1a9awZs0aent7efjhh9m8efNwogDo6Ohg\n2bJlHH744SxZsoQlS5awcOFC5s6dSybjGtYptW8r7H6EwkAvB3sOUhgcJJfLkstmaG1rI9vSAbl2\nyOSg0E8M9hHFIpnWDmjpgFwbRKQTC4j0HZJxsq3JMG3zoX0B5FprE3b/PnYe3Em+mGewOEghCkQE\nkc67GEWCINLYhg5CxDMHI0Ofh8YZyar5qzhizhE1ibmSuiUFSVngCuA1wFZgo6T1EfFAyWCXAHsj\n4nhJ5wF/A7y9XjHZ9LFjxw5+/OMf89BDD7FmzRrWrl1b1XjRW5jww3U6Ojo45ZRTOOWUUygWizz5\n5JNs27aNbdu2sX37dh5//PFD2h4ymQwLFixg3rx5zJkzh7lz59LR0TH8amtrG361trbS0tJCS0sL\nuVyOXC5HNpudUJyzUX7P4+z4/l9w1OPfJUeBLDD2IQIofU3UYKad/vYlFDqWEvOOgFwHPYU+ugYP\nkutczOFHnsTcxavIdR6WJKOWDiKK7Nj3OA8/dR+/2fsI27qeYH/vHjIhCoK8IE9QEBQk8gRFiQJB\nSBSBgqBI8jmUpK0ioiSdDXcfegG890Uf5i3Pf88klnhsiqiclSY1YemlwGUR8dr0+8cBIuLykmFu\nTYe5Q1IO2AksjVGCWrduXWzatGnc8bz9pivY0elbqM0Uox1NmdXe0PY22WrGyW23R/Y8yQ1v/sCE\nxpX0i4gYvdGO+lYfHQ1sKfm+FXhxpWEiIi9pH7AY2F06kKRLgUsBVq5cOaFgNOmVadNH6bGT2VSq\n5XY3kX1S/fdj9UwKI0Vf/otWMwwRcSVwJSQlhYkEc92bf38io5lZnQ0Ui3zwoS18+8m9nL9kLh9e\nMgeKRQqFpF4+k8mQyWTIZrPkcjlaWlqQRL67m94nfkt+3z7ajjicliVLKGZEz47t9G7dSs/OHfTv\n3U3f3j0MdO8nokBEkQJFWjvayS6YT/+Sxew4aiXbFh7FlpY59ChDPqA/X2RLpp1txfrtIhVBliAb\nkCHIANmI5HOACASHfH73EcfXLZ4h9UwKW4EVJd+XA9srDLM1rT5aADxdx5jMbJppzWT43HNXckRb\nC1f89ilu29fHwpYsC3JZOrIZchItEhIMFoN8JJWH7ZkM7fOOpGXBUfQXiwzuHSQfQbZ1CbnjlpI5\n/gVkJTIkR58DEfQVgp5igSf78+zoH6C3+Mwx5iJlOSyXIyfR2i5O6WjlXfM6WTuvgxXtrczPJTEV\nAw4UCuzLFziQL3KwUKA7X6RA0J7J0JZREltGtGUytKbvLRKtmWRZWjIiO03PeKtnUtgIrJZ0LLAN\nOA+4oGyY9cC7gDuAc4F/H609wcxmpozEnx63jOfOaefnXd3syxfYny/QVwgGo5gmghhOEAB7BwtJ\nMoigVcnONydRjCRx5CNpeyoGFAna0h12RybD8+Z18Jol81nW1sJzOts5aW4HS1tzVZ+a3J7NsLS1\npZ4/ScPULSmkbQTvB24lOSX1qxFxv6RPAZsiYj3wz8C3JG0mKSGcV694zGz6O/fIRZx75KJGhzGr\n1fU6hYjYAGwo6/bJks99wFvrGYOZmVXPV+aYmdkwJwUzMxvmpGBmZsOcFMzMbJiTgpmZDXNSMDOz\nYU4KZmY2rG53Sa0XSbuAJxocxhLKbtrXxGbKssyU5QAvy3TV7MtyTEQsHWugpksK04GkTdXcgrYZ\nzJRlmSnLAV6W6WomLctoXH1kZmbDnBTMzGyYk8LEXNnoAGpopizLTFkO8LJMVzNpWSpym4KZmQ1z\nScHMzIY5KZiZ2TAnBTMzG+akUAOSTpR0vaQvSjq30fFMlKRXSvqSpK9I+nmj45kMSadL+km6PKc3\nOp7JkPTcdDm+Lel/NDqeyZD0O5L+WdK3Gx1Lrcy0ZXJSqEDSVyU9Jek/yrqfKelhSZslfSzt/Drg\n8xHxP4Dfm/JgRzGe5YiIn0TE+4CbgW80It7RjHOdBNANtANbpzrWsYxzvTyYrpe3AdPu4qlxLstj\nEXFJYyJ9tnFuUyOabss0aRHh1wgv4HeBU4H/KOmWBR4FfgdoBe4FTgQOB64APgP8rNGxT3Q5Svpf\nD8xvdOyTXCeZtP8RwNWNjn2y6wU4G/g5cEGjY6/RNvbtRsc9gW3qeSQHTKWvw6fbMk325ZJCBRFx\nO/B0WefTgM2RHBkMANcBb4qIpyLiD4CPMc3ujTKe5QCQtBLYFxH7pzbSsY1znRTT/nuBtikMsyrj\nXS8RsT4iXga8Y2ojHdt4l2U6Gec29euIeEPZ66kpD7rOnBTG52hgS8n3rcDRklZJuhL4JklpYbob\ncTnSz5cAX5vyiCau0jp5i6QvA98C/rEhkY1fpWU5XdLn0uXZ0JjQxq3SsiyW9CXgBZI+3pjQxjTa\n/+NZmmSZqpZrdABNRiN0i4h4HLh0imOZjBGXAyAi/myKY5msSuvkRuDGqQ5mkioty4+AH01tKJNW\naVn2AO+b6mDGqeL/YyRNskxVc0lhfLYCK0q+Lwe2NyiWyZgpywFelumqmZelmWOfNCeF8dkIrJZ0\nrKRW4DxgfYNjmoiZshzgZZmumnlZmjn2SXNSqEDStcAdwAmStkq6JCLywPuBW4EHgesj4v5GxjmW\nmbIc4GWZrpp5WZo59nrxDfHMzGyYSwpmZjbMScHMzIY5KZiZ2TAnBTMzG+akYGZmw5wUzMxsmJOC\n2ThIelzSkskOYzZdOSmYmdkwJwWzCiTdJOkXku6XdGlZv1WSHpL0DUn3pU9F6ywZ5A8l/VLSryWt\nScc5TdLPJd2Tvp8wpQtkVgUnBbPKLo6IF5I87ewDkhaX9T8BuDIi1gL7gd8v6bc7Ik4Fvgh8OO32\nEPC7EfEC4JPAX9c1erMJcFIwq+wDku4F7iS5a+bqsv5bIuJn6eergFeU9Bu6bfcvgFXp5wXADemj\nH/8BOKkeQZtNhpOC2QgknQ6cAbw0Ip4P3EPyvOdS5TcOK/3en74XeOa5JX8B/DAiTgbeOML0zBrO\nScFsZAuAvRHRk7YJvGSEYVZKemn6+Xzgp1VMc1v6+aKaRGlWY04KZiP730BO0n0kR/h3jjDMg8C7\n0mEWkbQfjOZvgcsl/Yzk4fBm045vnW02AZJWATenVUFmM4ZLCmZmNswlBTMzG+aSgpmZDXNSMDOz\nYU4KZmY2zEnBzMyGOSmYmdkwJwUzMxv2fwFb2pprTF3bEAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f72f46aba20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a Lasso Object(set max_iter to 10000)\n",
    "# TODO Implement\n",
    "def lasso_reg(alphas,features,response):\n",
    "    lasso_model = Lasso(alpha=alphas,max_iter = 10000)\n",
    "    lasso_model.fit(features,response)\n",
    "    return lasso_model.coef_\n",
    "\n",
    "# Create list to hold lasso weights\n",
    "# TODO Implement\n",
    "Lasso_weights = []\n",
    "\n",
    "# Iterate over all alphas, performing data fitting with Lasso\n",
    "# and find the corresponding co-efficients\n",
    "# TODO Implement\n",
    "for alpha in lambdas:\n",
    "    Lasso_weights.append(lasso_reg(alpha,features,response))\n",
    "\n",
    "# Generate the plot\n",
    "# TODO Implement\n",
    "ax = plt.gca()\n",
    "ax.plot(lambdas, Lasso_weights)\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "#plt.axis('tight')\n",
    "\n",
    "# Name the plot\n",
    "# TODO Implement\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Lasso weights')\n",
    "plt.title('Lasso coefficients as a function of the regularization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now generate the same plot as above using Tensorflow\n",
    "# for the same set of lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment on the plots generated in problems $1$ and $2$ respectively.**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "From the plots, it is somewhat visible the difference between Ridge Regression ans Lasso. Lasso drives the coefficients to zero (feature selection), where Ridge Regression keeps (usually) all the coefficients (may be almost zero, but still present)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Impact of norms in the  Regularizer $~$ (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$4$. Assume$~$ $\\mathbf{x} \\in R^2$, $(x_1, x_2) \\in [-1, 1]\\times[-1, 1]$. $~$ (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, draw the contour plots for $\\mathbf{\\big|\\big|~x~\\big|\\big|_{0}}$, $\\mathbf{\\big|\\big|~x~\\big|\\big|_{1}}$, $\\mathbf{\\big|\\big|~x~\\big|\\big|_{2}}$ and $\\mathbf{\\big|\\big|~x~\\big|\\big|_{\\infty}}$ norms (consider all possible isolines in the given interval,i.e., ($[-1,1]\\times[-1,1]$) and **explain** how you get the corresponding plot, i.e., provide the mathematical formula for getting the outermost isoline in each case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://github.com/zshn25/Neural-Networks-Implementation-Application/blob/zee/img_norms.jpg?raw=true\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://github.com/zshn25/Neural-Networks-Implementation-Application/blob/zee/img_norms.jpg?raw=true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$5$. Sketch the **Lasso** optimization function, $~$ $\\mathbf{J(W) ~~=~~ \\big|\\big|~Y-XW~\\big|\\big|_{2}^2 ~+~\\lambda~ \\big|\\big|~W~\\big|\\big|_{1}}$ $~$ in two dimensions. From this sketch try to explain why **Lasso** induces **sparsity.** $~$ (2 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting to know Back-Propagation in details $~$ (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](https://github.com/mmarius/nnia-tutorial/blob/master/neural-net.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a **Feedforward Neural network** with one **input layer**, one **hidden layer** and one **output layer.** The **hidden layer** and **output layer** use the sigmoid function, $\\mathbf{\\sigma(x) = \\frac{1}{1+exp(-x)}}$, as **activation function.** Also note, that the network minimizes **Binary Cross Entropy loss**, given by, $$\\mathbf{J = \\sum -y\\log(\\hat{y}) - (1-y)\\log(1-\\hat{y})}$$\n",
    "\n",
    "We consider the true class labels to be **binary**, i.e., either $0$ or $1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the purpose of computing the derivatives of the loss/cost function consider the numerical values obtained by the network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input layer** consists of two nodes, $x_1$ and $x_2$ respectively. For our problem consider the following input,\n",
    "$$\\begin{bmatrix} x_1\\\\ x_2\\\\ \\end{bmatrix} = \\begin{bmatrix} -1\\\\ 1\\\\ \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden layer** is made up of 3 neurons and the corresponding matrix of weights is as given:\n",
    "$$\n",
    "\\mathbf{W_{hidden}}\n",
    "~=~\n",
    "\\begin{bmatrix} w_1^{1} & w_1^{2} & w_1^{3} \\\\ w_2^{1} & w_2^{2} & w_2^{3} \\end{bmatrix}\n",
    "~=~\n",
    "\\begin{bmatrix} 0.15 & -0.25 & 0.05\\\\ 0.20 & 0.10 & -0.15 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Note:** Output of **Hidden layer** is given by, $~~$ $\\mathbf{a=\\sigma~(W_{hidden}^{T}x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output layer** consists of one neuron, i.e., the **network** generates a single output. **For our problem, the true class label is $1$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix corresponding to the **Output layer** is given by,\n",
    "$$\n",
    "\\mathbf{W_{out}}\n",
    "~=~\n",
    "\\begin{bmatrix} w_1\\\\w_2 \\\\w_3\\end{bmatrix}\n",
    "~=~\n",
    "\\begin{bmatrix} 0.20\\\\-0.35\\\\0.15 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Note:** output from the **Output layer** is given by, $~$ $\\mathbf{\\hat{y} = \\sigma~(W_{out}^Ta)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$6$. Execute the following sequence of operations and **show that Binary Cross Entropy loss is getting reduced, i.e., $ C^2 < C^1$** $~$ ($3$ points)\n",
    "\n",
    "**Perform Forward-propagation to generate output** $\\to$ **Compute loss or cost ($C^1$)** $\\to$ **perform Back-propagation to compute the error** $\\to$ **perform Gradient descent to update the weights** $\\to$ **peform Forward-propagation again with updated weights** $\\to$ **Compute loss or cost ($C^2$)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**  $C^i$ denotes the loss or cost at the $i^{th}$ iteration, for performing Gradient descent consider a learning rate of $0.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "In the hidden layer we have:\n",
    "\n",
    "$$\n",
    "Z_{in} = W_{hidden}^T x \n",
    "~=~\n",
    "\\begin{bmatrix} 0.15 & 0.20 \\\\ -0.25 & 0.10 \\\\ 0.05 & -0.15\\end{bmatrix}\n",
    "\\begin{bmatrix} -1\\\\ 1\\\\ \\end{bmatrix}\n",
    "~=~\n",
    "\\begin{bmatrix} 0.05\\\\ 0.35\\\\ -0.20 \\end{bmatrix}\n",
    "~\n",
    "\\implies\n",
    "~\n",
    "a = \\sigma(Z_{in}) ~\\approx~ \\begin{bmatrix} 0.512\\\\ 0.587\\\\ 0.450\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "In the output layer we have:\n",
    "\n",
    "$$\n",
    "Z_{out = }W_{out}^T a = \n",
    "\\begin{bmatrix} 0.20 & -0.35 & 0.15\\end{bmatrix}\n",
    "\\begin{bmatrix} 0.512\\\\ 0.587\\\\ 0.450\\end{bmatrix}\n",
    "\\approx\n",
    "-0.035\n",
    "\\implies\n",
    "\\hat{y} = \\sigma(Z_{out}) \\approx 0.491\n",
    "$$\n",
    "\n",
    "And with this result and $y = 1$ we can compute the cost of the first pass as\n",
    "\n",
    "$$\n",
    "J = \\sum -(1)\\log(0.491) - (1-1)\\log(1-0.491) \\approx 0.711 = C_1\n",
    "$$\n",
    "\n",
    "Now, we start with the backward pass. To that end, we start by computing the derivatives of the cost function w.r.t. $W_{out}$ so that we can update those weights using a gradient descent scheme:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W_{out}} = \\frac{\\partial J}{\\partial \\hat{y}} \n",
    "~\n",
    "\\frac{\\partial \\hat{y}}{\\partial Z_{out}} \n",
    "~\n",
    "\\frac{\\partial Z_{out}}{\\partial W_{out}}\n",
    "~\n",
    "=\n",
    "~\n",
    "\\left( \\frac{-y+\\hat{y}}{\\hat{y}-\\hat{y}^2} \\right)\n",
    "~\n",
    "(\\sigma(Z_{out})(1-\\sigma(Z_{out})))\n",
    "~\n",
    "a\n",
    "=\n",
    "~\n",
    "\\delta_{out}\n",
    "~\n",
    "a\n",
    "\\approx (-0.509) \\begin{bmatrix} 0.512\\\\ 0.587\\\\ 0.450\\end{bmatrix}\n",
    "\\approx \\begin{bmatrix} -0.261\\\\ -0.298\\\\ 0.229\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then we update the output weights with the gradient descent scheme\n",
    "\n",
    "$$\n",
    "W_{out} = W_{out} - \\alpha \\frac{\\partial J}{\\partial W_{out}} = \\begin{bmatrix} 0.20 \\\\ -0.35 \\\\ 0.15\\end{bmatrix} - (0.1)\\begin{bmatrix} 0.20 \\\\ -0.35 \\\\ 0.15\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 0.226 \\\\ -0.320 \\\\ 0.173\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then, we can procede to compute the derivative of the cost function $J$ w.r.t. $W_{hidden}$ in order to update them. The derivative is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W_{hidden}} = \\frac{\\partial J}{\\partial \\hat{y}} \n",
    "~\n",
    "\\frac{\\partial \\hat{y}}{\\partial Z_{out}} \n",
    "~\n",
    "\\frac{\\partial Z_{out}}{\\partial a}\n",
    "~\n",
    "\\frac{\\partial a}{\\partial Z_{in}}\n",
    "~\n",
    "\\frac{\\partial Z_{in}}{\\partial W_{in}}\n",
    "~\n",
    "=\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta_{out}\n",
    "~\n",
    "W_{out}\n",
    "~\n",
    "\\frac{\\partial Z_{out}}{\\partial a}\n",
    "~\n",
    "\\frac{\\partial a}{\\partial Z_{in}}\n",
    "~\n",
    "\\frac{\\partial Z_{in}}{\\partial W_{in}}\n",
    "~\n",
    "=\n",
    "~\n",
    "\\delta_{out}~W_{out}~\\sigma(Z_{in})(1-\\sigma(Z_{in}))(x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies\n",
    "\\delta_{in} \\frac{\\partial Z_{in}}{\\partial W_{in}}\n",
    "~\n",
    "\\approx\n",
    "~\n",
    "\\begin{bmatrix} 0.029 & -0.028 \\\\ -0.04 & 0.04 \\\\ 0.022 & -0.022\\end{bmatrix}\n",
    "\\begin{bmatrix} -1 & 1\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "With the transpose of the previous result, we can update $W_{hidden}$ with the gradient descent scheme\n",
    "\n",
    "$$\n",
    "W_{hidden} = W_{hidden} - \\alpha \\frac{\\partial J}{\\partial W_{hidden}} \n",
    "~\n",
    "=\n",
    "~\n",
    "\\begin{bmatrix} 0.15 & -0.25 & 0.05\\\\ 0.20 & 0.10 & -0.15 \\end{bmatrix}\n",
    "~\n",
    "- (0.1) \\begin{bmatrix} 0.029 & -0.04 & 0.022\\\\ -0.028 & 0.04 & -0.022 \\end{bmatrix}\n",
    "~\n",
    "=\n",
    "~\n",
    "\\begin{bmatrix} 0.147 & -0.246 & 0.047\\\\ 0.203 & 0.096 & -0.148 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And now, we compute the forward pass with the updated results. In the hidden layer we have:\n",
    "\n",
    "$$\n",
    "Z_{in} = W_{hidden}^T x \n",
    "~=~\n",
    "\\begin{bmatrix} 0.147 & -0.246 & 0.047\\\\ 0.203 & 0.096 & -0.148 \\end{bmatrix}\n",
    "\\begin{bmatrix} -1\\\\ 1\\\\ \\end{bmatrix}\n",
    "~=~\n",
    "\\begin{bmatrix} 0.056\\\\ 0.342\\\\ -0.197 \\end{bmatrix}\n",
    "~\n",
    "\\implies\n",
    "~\n",
    "a = \\sigma(Z_{in}) ~\\approx~ \\begin{bmatrix} 0.514\\\\ 0.585\\\\ 0.451\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "In the output layer we have:\n",
    "\n",
    "$$\n",
    "Z_{out = }W_{out}^T a = \n",
    "\\begin{bmatrix} 0.226 & -0.320 & 0.173\\end{bmatrix}\n",
    "\\begin{bmatrix} 0.514\\\\ 0.585\\\\ 0.451\\end{bmatrix}\n",
    "\\approx\n",
    "0.007\n",
    "\\implies\n",
    "\\hat{y} = \\sigma(Z_{out}) \\approx 0.502\n",
    "$$\n",
    "\n",
    "And with this result and $y = 1$ we can compute the cost of the first pass as\n",
    "\n",
    "$$\n",
    "J = \\sum -(1)\\log(0.491) - (1-1)\\log(1-0.491) \\approx 0.689 = C_2\n",
    "$$\n",
    "\n",
    "And from the previous computations we can see that $C_2 < C_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward Neural Network with L1 and L2 regularization $~$ (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you would build a **feed-forward network** from scratch using only **Numpy** in python. For this, you also have to implement **Back-propagation** in python. Additionally, this network should have the option of **L1 and L2 regularization** enabled within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download **mnist** dataset from NNIA piazza page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    " \n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"function for loading data\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, \n",
    "                               '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', \n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, \n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", \n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, \n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_mnist('mnist/', kind='train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 10000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = load_mnist('mnist/', kind='t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-60-76b946074f63>, line 63)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-60-76b946074f63>\"\u001b[0;36m, line \u001b[0;32m63\u001b[0m\n\u001b[0;31m    def encode_labels(self, y, k):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\" Feedforward neural network with a single hidden layer.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of output units, should be equal to the\n",
    "        number of unique class labels.\n",
    "        \n",
    "    n_features : int\n",
    "        Number of features (dimensions) in the target dataset.\n",
    "        Should be equal to the number of columns in the X array.\n",
    "        \n",
    "    n_hidden : int\n",
    "        Number of hidden units.\n",
    "        \n",
    "    l1 : float\n",
    "        Regularizer for L1-regularization.\n",
    "        l1=0.0 implies no regularization\n",
    "        \n",
    "    l2 : float\n",
    "        Lambda value for L2-regularization.\n",
    "        l2=0.0 implies no regularization\n",
    "        \n",
    "    epochs : int\n",
    "        Number of passes over the training set.\n",
    "        \n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate.\n",
    "        \n",
    "    decrease_const : float (default: 0.0)\n",
    "        Decrease constant. Shrinks the learning rate\n",
    "        after each epoch via eta / (1 + epoch*decrease_const)\n",
    "        \n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch if True to prevent circles.\n",
    "        \n",
    "    minibatches : int (default: 1)\n",
    "        Divides training data into k minibatches for efficiency.\n",
    "        \n",
    "    random_state : int (default: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    cost_ : list\n",
    "      Sum of squared errors after each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l1=0.0, l2=0.0, epochs=50, eta=0.001,\n",
    "                 decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, random_state=None):\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "\n",
    "\n",
    "    def encode_labels(self, y, k):\n",
    "        \"\"\"Encode the labels using one-hot representation\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : y represents target values.\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot array\n",
    "\n",
    "        \"\"\"\n",
    "        #TODO Implement\n",
    "        \n",
    "        \n",
    "\n",
    "    def initialize_weights(self):\n",
    "        \"\"\"Initialize using random numbers.\"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Compute sigmoid function\n",
    "           Implement a stable version which \n",
    "           takes care of overflow and underflow.\n",
    "        \"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "        \n",
    "\n",
    "    def sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the sigmoid function\"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "        \n",
    "\n",
    "    def add_bias_unit(self, X, how='column'):\n",
    "        \"\"\"Add bias unit to array at index 0\"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "        \n",
    "\n",
    "    def feedforward(self, X, w1, w2):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        a1 : array,\n",
    "            Input values with bias unit.\n",
    "        z2 : array,\n",
    "            Net input of hidden layer.\n",
    "        a2 : array,\n",
    "            Activation of hidden layer.\n",
    "        z3 : array,\n",
    "            Net input of output layer.\n",
    "        a3 : array,\n",
    "            Activation of output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement\n",
    "        \n",
    "        \n",
    "\n",
    "    def L2_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "        \n",
    "\n",
    "    def L1_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L1-regularization cost\"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "        \n",
    "    def get_cost(self, y_enc, output, w1, w2):\n",
    "        \"\"\"Compute cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, one-hot encoded class labels.\n",
    "        \n",
    "        output : array, Activation of the output layer (feedforward)\n",
    "        \n",
    "        w1 : array, Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float, Regularized cost.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "        \n",
    "\n",
    "    def get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        a1 : array, Input values with bias unit.\n",
    "        a2 : array, Activation of hidden layer.\n",
    "        a3 : array, Activation of output layer.\n",
    "        z2 : array, Net input of hidden layer.\n",
    "        y_enc : array, one-hot encoded class labels.\n",
    "        w1 : array, Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, Gradient of the weight matrix w1.\n",
    "        grad2 : array, Gradient of the weight matrix w2.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # backpropagation\n",
    "        #TODO Implement\n",
    "        \n",
    "        \n",
    "\n",
    "        # regularize\n",
    "        #TODO Implement\n",
    "        \n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input layer with original features.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, Predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO Implement\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input layer with original features.\n",
    "        y : array, Target class labels.\n",
    "        print_progress : bool, Prints the progress\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        #TODO Implement\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = MLP(n_output=10, \n",
    "                  n_features=X_train.shape[1], \n",
    "                  n_hidden=50, \n",
    "                  l2=0.1, \n",
    "                  l1=0.0, \n",
    "                  epochs=1000, \n",
    "                  eta=0.001,\n",
    "                  decrease_const=0.00001,\n",
    "                  minibatches=50, \n",
    "                  shuffle=True,\n",
    "                  random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nn.fit(X_train, y_train, print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training error for every iteration\n",
    "# in every epoch\n",
    "\n",
    "# TODO Implement\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the training error in every epoch\n",
    "# TODO Implement\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute Training Accuracy\n",
    "# TODO Implement\n",
    "\n",
    "print('Training accuracy: %.2f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute Test Accuracy\n",
    "# TODO Implement\n",
    "\n",
    "print('Test accuracy: %.2f%%' % (acc * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
